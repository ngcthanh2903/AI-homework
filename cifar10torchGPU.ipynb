{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP7MxdoXj4JTv1kzaIzFDkv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngcthanh2903/AI-homework/blob/main/cifar10torchGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "qypfIwICYrKF"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import cifar10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) =cifar10.load_data()\n",
        "print(x_test.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LAEjM7gZS9R",
        "outputId": "116df64b-4e4a-4520-915d-c75c6c2a522f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uint8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=x_train.reshape(-1,3,32,32)\n",
        "x_test=x_test.reshape(-1,3,32,32)\n"
      ],
      "metadata": {
        "id": "_OGWSIOvZsuW"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seL4uDNpaZup",
        "outputId": "4ae1d185-9bdf-4b78-b403-1961587596e6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 3, 32, 32)\n",
            "(10000, 3, 32, 32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.adam as adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import TensorDataset\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "Vt8mn0h3alQf"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = torch.from_numpy(x_train)\n",
        "y_train = torch.from_numpy(y_train)\n",
        "x_test = torch.from_numpy(x_test)\n",
        "y_test = torch.from_numpy(y_test)\n",
        "x_train = x_train.to(torch.float32)\n",
        "x_test = x_test.to(torch.float32)"
      ],
      "metadata": {
        "id": "PkqEs0XscgVn"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =64\n",
        "train_ds = TensorDataset(x_train,y_train)\n",
        "test_ds = TensorDataset(x_test,y_test)\n",
        "train_dl = DataLoader(train_ds,batch_size,shuffle=True)\n",
        "test_dl = DataLoader(test_ds,batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "Im4d1ajZh5Vd"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR1BctsI6aUL",
        "outputId": "e5c1a5fa-0772-4a7b-efcf-9699033654ef"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50000, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3,padding=2)\n",
        "        self.conv2 = nn.Conv2d(32,64,kernel_size= 3,padding=2)\n",
        "        self.conv3 = nn.Conv2d(64,64,kernel_size=3,padding=2)\n",
        "        self.conv3_drop = nn.Dropout2d()\n",
        "        self.conv4 = nn.Conv2d(64,128,kernel_size=3,padding=2)\n",
        "        self.fc1 = nn.Linear(1152, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv3_drop(self.conv3(x)), 2))\n",
        "        x = F.relu(F.max_pool2d(self.conv4(x), 2))\n",
        "        #print(x.shape)\n",
        "        #print(x.reshape(x.shape[0],-1).shape)\n",
        "        x = x.view(-1, 1152)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x)"
      ],
      "metadata": {
        "id": "o59tizgciA_K"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "model = Net()\n",
        "model.cuda()\n",
        "\n",
        "summary(model, (3, 32, 32))\n",
        "loss_fn = CrossEntropyLoss()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8GSanp5iLgS",
        "outputId": "fe8a288b-c337-447e-821b-d69e56e2de5f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 34, 34]             896\n",
            "            Conv2d-2           [-1, 64, 19, 19]          18,496\n",
            "            Conv2d-3           [-1, 64, 11, 11]          36,928\n",
            "         Dropout2d-4           [-1, 64, 11, 11]               0\n",
            "            Conv2d-5            [-1, 128, 7, 7]          73,856\n",
            "            Linear-6                  [-1, 100]         115,300\n",
            "            Linear-7                   [-1, 10]           1,010\n",
            "================================================================\n",
            "Total params: 246,486\n",
            "Trainable params: 246,486\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.63\n",
            "Params size (MB): 0.94\n",
            "Estimated Total Size (MB): 1.58\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential()\n",
        "model.add_module('conv1',nn.Conv2d(3,32,kernel_size=3,padding = 'same'))\n",
        "model.add_module('relu1',nn.ReLU())\n",
        "model.add_module('conv2',nn.Conv2d(32,32,kernel_size=3,padding='same'))\n",
        "model.add_module('relu2',nn.ReLU())\n",
        "model.add_module('pool1',nn.MaxPool2d(2))\n",
        "model.add_module('bn1',nn.BatchNorm2d(32))\n",
        "\n",
        "model.add_module('conv3',nn.Conv2d(32,64,kernel_size=3,padding = 'same'))\n",
        "model.add_module('relu3',nn.ReLU())\n",
        "model.add_module('conv4',nn.Conv2d(64,64,kernel_size=3,padding='same'))\n",
        "model.add_module('relu4',nn.ReLU())\n",
        "model.add_module('pool2',nn.MaxPool2d(2))\n",
        "model.add_module('bn2',nn.BatchNorm2d(64))\n",
        "\n",
        "model.add_module('conv5',nn.Conv2d(64,128,kernel_size=3,padding = 'same'))\n",
        "model.add_module('relu5',nn.ReLU())\n",
        "model.add_module('conv6',nn.Conv2d(128,128,kernel_size=3,padding='same'))\n",
        "model.add_module('relu6',nn.ReLU())\n",
        "model.add_module('pool3',nn.MaxPool2d(2))\n",
        "model.add_module('bn3',nn.BatchNorm2d(128))\n",
        "\n",
        "model.add_module('conv7',nn.Conv2d(128,256,kernel_size=3,padding = 'same'))\n",
        "model.add_module('relu7',nn.ReLU())\n",
        "model.add_module('conv8',nn.Conv2d(256,256,kernel_size=3,padding='same'))\n",
        "model.add_module('relu8',nn.ReLU())\n",
        "model.add_module('pool4',nn.MaxPool2d(2))\n",
        "model.add_module('bn4',nn.BatchNorm2d(256))\n",
        "\n",
        "\n",
        "model.add_module('flatten',nn.Flatten())\n",
        "#x= torch.ones((64, 3, 32, 32))\n",
        "#model(x).shape\n",
        "model.add_module('dropout',nn.Dropout(p=0.25))\n",
        "model.add_module('dense',nn.Linear(1024,252))\n",
        "model.add_module('dene2',nn.Linear(252,10))\n",
        "model(x).shape\n",
        "from torchsummary import summary\n",
        "model.cuda()\n",
        "summary(model, (3, 32, 32))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFa_UWm0Yjuj",
        "outputId": "0a984491-356b-4dfc-d258-f553e8951912"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             896\n",
            "              ReLU-2           [-1, 32, 32, 32]               0\n",
            "            Conv2d-3           [-1, 32, 32, 32]           9,248\n",
            "              ReLU-4           [-1, 32, 32, 32]               0\n",
            "         MaxPool2d-5           [-1, 32, 16, 16]               0\n",
            "       BatchNorm2d-6           [-1, 32, 16, 16]              64\n",
            "            Conv2d-7           [-1, 64, 16, 16]          18,496\n",
            "              ReLU-8           [-1, 64, 16, 16]               0\n",
            "            Conv2d-9           [-1, 64, 16, 16]          36,928\n",
            "             ReLU-10           [-1, 64, 16, 16]               0\n",
            "        MaxPool2d-11             [-1, 64, 8, 8]               0\n",
            "      BatchNorm2d-12             [-1, 64, 8, 8]             128\n",
            "           Conv2d-13            [-1, 128, 8, 8]          73,856\n",
            "             ReLU-14            [-1, 128, 8, 8]               0\n",
            "           Conv2d-15            [-1, 128, 8, 8]         147,584\n",
            "             ReLU-16            [-1, 128, 8, 8]               0\n",
            "        MaxPool2d-17            [-1, 128, 4, 4]               0\n",
            "      BatchNorm2d-18            [-1, 128, 4, 4]             256\n",
            "           Conv2d-19            [-1, 256, 4, 4]         295,168\n",
            "             ReLU-20            [-1, 256, 4, 4]               0\n",
            "           Conv2d-21            [-1, 256, 4, 4]         590,080\n",
            "             ReLU-22            [-1, 256, 4, 4]               0\n",
            "        MaxPool2d-23            [-1, 256, 2, 2]               0\n",
            "      BatchNorm2d-24            [-1, 256, 2, 2]             512\n",
            "          Flatten-25                 [-1, 1024]               0\n",
            "          Dropout-26                 [-1, 1024]               0\n",
            "           Linear-27                  [-1, 252]         258,300\n",
            "           Linear-28                   [-1, 10]           2,530\n",
            "================================================================\n",
            "Total params: 1,434,046\n",
            "Trainable params: 1,434,046\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 2.13\n",
            "Params size (MB): 5.47\n",
            "Estimated Total Size (MB): 7.61\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "rFHcWDGFiYBw"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 10\n",
        "log_interval = 10\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.manual_seed(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGHfwgpY56UM",
        "outputId": "de141e26-0438-497f-aa5c-3bf00ac39f53"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6c82f80fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "ktKheBlvsEyf"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(n_epochs+1):\n",
        "  model.train()\n",
        "  correct = 0\n",
        "  for batch_idx, (data, target) in enumerate(train_dl):\n",
        "    data = data.type(torch.cuda.FloatTensor)\n",
        "    #target = target.type(torch.cuda.FloatTensor)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    target = target.squeeze(1)\n",
        "    #output, target = output.type(torch.cuda.FloatTensor), target.type(torch.cuda.FloatTensor)\n",
        "    #loss = F.nll_loss(output, target)\n",
        "    #correct += output.eq(target.data.view_as(output)).sum()\n",
        "    loss = loss_fn(output,target.to(device))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_dl.dataset),100. * batch_idx / len(train_dl), loss.item()))\n",
        "      torch.save(model.state_dict(), 'model.pth')\n",
        "      torch.save(optimizer.state_dict(), 'optimizer.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxoDD2Y05Y4D",
        "outputId": "d1d9f407-3e5e-46ab-aada-84894632e627"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/50000 (0%)]\tLoss: 2.369097\n",
            "Train Epoch: 0 [640/50000 (1%)]\tLoss: 2.045074\n",
            "Train Epoch: 0 [1280/50000 (3%)]\tLoss: 2.009960\n",
            "Train Epoch: 0 [1920/50000 (4%)]\tLoss: 2.126073\n",
            "Train Epoch: 0 [2560/50000 (5%)]\tLoss: 2.318980\n",
            "Train Epoch: 0 [3200/50000 (6%)]\tLoss: 2.442873\n",
            "Train Epoch: 0 [3840/50000 (8%)]\tLoss: 2.181601\n",
            "Train Epoch: 0 [4480/50000 (9%)]\tLoss: 1.940383\n",
            "Train Epoch: 0 [5120/50000 (10%)]\tLoss: 2.212939\n",
            "Train Epoch: 0 [5760/50000 (12%)]\tLoss: 2.137083\n",
            "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 1.880618\n",
            "Train Epoch: 0 [7040/50000 (14%)]\tLoss: 2.226659\n",
            "Train Epoch: 0 [7680/50000 (15%)]\tLoss: 2.129822\n",
            "Train Epoch: 0 [8320/50000 (17%)]\tLoss: 1.914973\n",
            "Train Epoch: 0 [8960/50000 (18%)]\tLoss: 1.995107\n",
            "Train Epoch: 0 [9600/50000 (19%)]\tLoss: 1.992796\n",
            "Train Epoch: 0 [10240/50000 (20%)]\tLoss: 1.980098\n",
            "Train Epoch: 0 [10880/50000 (22%)]\tLoss: 1.800771\n",
            "Train Epoch: 0 [11520/50000 (23%)]\tLoss: 1.889899\n",
            "Train Epoch: 0 [12160/50000 (24%)]\tLoss: 1.933161\n",
            "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 1.861872\n",
            "Train Epoch: 0 [13440/50000 (27%)]\tLoss: 1.785108\n",
            "Train Epoch: 0 [14080/50000 (28%)]\tLoss: 1.865967\n",
            "Train Epoch: 0 [14720/50000 (29%)]\tLoss: 2.025707\n",
            "Train Epoch: 0 [15360/50000 (31%)]\tLoss: 1.809720\n",
            "Train Epoch: 0 [16000/50000 (32%)]\tLoss: 1.769619\n",
            "Train Epoch: 0 [16640/50000 (33%)]\tLoss: 1.752261\n",
            "Train Epoch: 0 [17280/50000 (35%)]\tLoss: 1.885570\n",
            "Train Epoch: 0 [17920/50000 (36%)]\tLoss: 1.807082\n",
            "Train Epoch: 0 [18560/50000 (37%)]\tLoss: 1.784823\n",
            "Train Epoch: 0 [19200/50000 (38%)]\tLoss: 1.716010\n",
            "Train Epoch: 0 [19840/50000 (40%)]\tLoss: 1.752891\n",
            "Train Epoch: 0 [20480/50000 (41%)]\tLoss: 1.919036\n",
            "Train Epoch: 0 [21120/50000 (42%)]\tLoss: 1.726441\n",
            "Train Epoch: 0 [21760/50000 (43%)]\tLoss: 1.888393\n",
            "Train Epoch: 0 [22400/50000 (45%)]\tLoss: 1.728705\n",
            "Train Epoch: 0 [23040/50000 (46%)]\tLoss: 1.663643\n",
            "Train Epoch: 0 [23680/50000 (47%)]\tLoss: 1.838857\n",
            "Train Epoch: 0 [24320/50000 (49%)]\tLoss: 1.897292\n",
            "Train Epoch: 0 [24960/50000 (50%)]\tLoss: 1.690689\n",
            "Train Epoch: 0 [25600/50000 (51%)]\tLoss: 1.904094\n",
            "Train Epoch: 0 [26240/50000 (52%)]\tLoss: 1.896416\n",
            "Train Epoch: 0 [26880/50000 (54%)]\tLoss: 1.664147\n",
            "Train Epoch: 0 [27520/50000 (55%)]\tLoss: 1.620977\n",
            "Train Epoch: 0 [28160/50000 (56%)]\tLoss: 1.763700\n",
            "Train Epoch: 0 [28800/50000 (58%)]\tLoss: 1.466751\n",
            "Train Epoch: 0 [29440/50000 (59%)]\tLoss: 1.922410\n",
            "Train Epoch: 0 [30080/50000 (60%)]\tLoss: 1.724065\n",
            "Train Epoch: 0 [30720/50000 (61%)]\tLoss: 1.547619\n",
            "Train Epoch: 0 [31360/50000 (63%)]\tLoss: 1.690139\n",
            "Train Epoch: 0 [32000/50000 (64%)]\tLoss: 1.670692\n",
            "Train Epoch: 0 [32640/50000 (65%)]\tLoss: 1.592594\n",
            "Train Epoch: 0 [33280/50000 (66%)]\tLoss: 1.894933\n",
            "Train Epoch: 0 [33920/50000 (68%)]\tLoss: 1.599180\n",
            "Train Epoch: 0 [34560/50000 (69%)]\tLoss: 1.951004\n",
            "Train Epoch: 0 [35200/50000 (70%)]\tLoss: 1.901372\n",
            "Train Epoch: 0 [35840/50000 (72%)]\tLoss: 1.878888\n",
            "Train Epoch: 0 [36480/50000 (73%)]\tLoss: 1.802045\n",
            "Train Epoch: 0 [37120/50000 (74%)]\tLoss: 1.669719\n",
            "Train Epoch: 0 [37760/50000 (75%)]\tLoss: 1.845778\n",
            "Train Epoch: 0 [38400/50000 (77%)]\tLoss: 1.660094\n",
            "Train Epoch: 0 [39040/50000 (78%)]\tLoss: 1.656829\n",
            "Train Epoch: 0 [39680/50000 (79%)]\tLoss: 1.507313\n",
            "Train Epoch: 0 [40320/50000 (81%)]\tLoss: 1.859146\n",
            "Train Epoch: 0 [40960/50000 (82%)]\tLoss: 1.545742\n",
            "Train Epoch: 0 [41600/50000 (83%)]\tLoss: 1.982458\n",
            "Train Epoch: 0 [42240/50000 (84%)]\tLoss: 1.671511\n",
            "Train Epoch: 0 [42880/50000 (86%)]\tLoss: 1.720623\n",
            "Train Epoch: 0 [43520/50000 (87%)]\tLoss: 1.606681\n",
            "Train Epoch: 0 [44160/50000 (88%)]\tLoss: 1.789319\n",
            "Train Epoch: 0 [44800/50000 (90%)]\tLoss: 1.700304\n",
            "Train Epoch: 0 [45440/50000 (91%)]\tLoss: 1.651160\n",
            "Train Epoch: 0 [46080/50000 (92%)]\tLoss: 1.717961\n",
            "Train Epoch: 0 [46720/50000 (93%)]\tLoss: 1.719392\n",
            "Train Epoch: 0 [47360/50000 (95%)]\tLoss: 1.515339\n",
            "Train Epoch: 0 [48000/50000 (96%)]\tLoss: 1.456997\n",
            "Train Epoch: 0 [48640/50000 (97%)]\tLoss: 1.740890\n",
            "Train Epoch: 0 [49280/50000 (98%)]\tLoss: 2.094911\n",
            "Train Epoch: 0 [49920/50000 (100%)]\tLoss: 1.684606\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 1.590076\n",
            "Train Epoch: 1 [640/50000 (1%)]\tLoss: 1.660554\n",
            "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 1.761300\n",
            "Train Epoch: 1 [1920/50000 (4%)]\tLoss: 1.702483\n",
            "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 1.706764\n",
            "Train Epoch: 1 [3200/50000 (6%)]\tLoss: 1.528976\n",
            "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 1.689695\n",
            "Train Epoch: 1 [4480/50000 (9%)]\tLoss: 1.561175\n",
            "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 1.574154\n",
            "Train Epoch: 1 [5760/50000 (12%)]\tLoss: 2.132867\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 1.553251\n",
            "Train Epoch: 1 [7040/50000 (14%)]\tLoss: 1.470343\n",
            "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 1.654456\n",
            "Train Epoch: 1 [8320/50000 (17%)]\tLoss: 1.558234\n",
            "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 1.618088\n",
            "Train Epoch: 1 [9600/50000 (19%)]\tLoss: 1.600377\n",
            "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.533091\n",
            "Train Epoch: 1 [10880/50000 (22%)]\tLoss: 1.725849\n",
            "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 1.487174\n",
            "Train Epoch: 1 [12160/50000 (24%)]\tLoss: 1.674027\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.577916\n",
            "Train Epoch: 1 [13440/50000 (27%)]\tLoss: 1.645232\n",
            "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 1.526358\n",
            "Train Epoch: 1 [14720/50000 (29%)]\tLoss: 1.480386\n",
            "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.640729\n",
            "Train Epoch: 1 [16000/50000 (32%)]\tLoss: 1.648041\n",
            "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 1.659553\n",
            "Train Epoch: 1 [17280/50000 (35%)]\tLoss: 1.763963\n",
            "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.559310\n",
            "Train Epoch: 1 [18560/50000 (37%)]\tLoss: 1.818125\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 1.521176\n",
            "Train Epoch: 1 [19840/50000 (40%)]\tLoss: 1.677557\n",
            "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.484076\n",
            "Train Epoch: 1 [21120/50000 (42%)]\tLoss: 1.741167\n",
            "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 1.578978\n",
            "Train Epoch: 1 [22400/50000 (45%)]\tLoss: 1.405977\n",
            "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.657157\n",
            "Train Epoch: 1 [23680/50000 (47%)]\tLoss: 1.654715\n",
            "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.433003\n",
            "Train Epoch: 1 [24960/50000 (50%)]\tLoss: 1.663219\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.661702\n",
            "Train Epoch: 1 [26240/50000 (52%)]\tLoss: 1.616575\n",
            "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 1.669417\n",
            "Train Epoch: 1 [27520/50000 (55%)]\tLoss: 1.350718\n",
            "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.885642\n",
            "Train Epoch: 1 [28800/50000 (58%)]\tLoss: 1.638220\n",
            "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 1.759384\n",
            "Train Epoch: 1 [30080/50000 (60%)]\tLoss: 1.513460\n",
            "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.526045\n",
            "Train Epoch: 1 [31360/50000 (63%)]\tLoss: 1.576658\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.480147\n",
            "Train Epoch: 1 [32640/50000 (65%)]\tLoss: 1.507723\n",
            "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.622119\n",
            "Train Epoch: 1 [33920/50000 (68%)]\tLoss: 1.447099\n",
            "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 1.449078\n",
            "Train Epoch: 1 [35200/50000 (70%)]\tLoss: 1.432471\n",
            "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.578113\n",
            "Train Epoch: 1 [36480/50000 (73%)]\tLoss: 1.588125\n",
            "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 1.665753\n",
            "Train Epoch: 1 [37760/50000 (75%)]\tLoss: 1.624907\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.456624\n",
            "Train Epoch: 1 [39040/50000 (78%)]\tLoss: 1.531524\n",
            "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 1.962629\n",
            "Train Epoch: 1 [40320/50000 (81%)]\tLoss: 1.597649\n",
            "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.264869\n",
            "Train Epoch: 1 [41600/50000 (83%)]\tLoss: 1.368000\n",
            "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 1.539325\n",
            "Train Epoch: 1 [42880/50000 (86%)]\tLoss: 1.576599\n",
            "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.424830\n",
            "Train Epoch: 1 [44160/50000 (88%)]\tLoss: 1.287768\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.559841\n",
            "Train Epoch: 1 [45440/50000 (91%)]\tLoss: 1.488769\n",
            "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.597391\n",
            "Train Epoch: 1 [46720/50000 (93%)]\tLoss: 1.666973\n",
            "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 1.481290\n",
            "Train Epoch: 1 [48000/50000 (96%)]\tLoss: 1.467987\n",
            "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.569738\n",
            "Train Epoch: 1 [49280/50000 (98%)]\tLoss: 1.699239\n",
            "Train Epoch: 1 [49920/50000 (100%)]\tLoss: 1.452173\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.605922\n",
            "Train Epoch: 2 [640/50000 (1%)]\tLoss: 1.424571\n",
            "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 1.628727\n",
            "Train Epoch: 2 [1920/50000 (4%)]\tLoss: 1.319813\n",
            "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.643513\n",
            "Train Epoch: 2 [3200/50000 (6%)]\tLoss: 1.440762\n",
            "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 1.398558\n",
            "Train Epoch: 2 [4480/50000 (9%)]\tLoss: 1.317627\n",
            "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.355170\n",
            "Train Epoch: 2 [5760/50000 (12%)]\tLoss: 1.474897\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.586376\n",
            "Train Epoch: 2 [7040/50000 (14%)]\tLoss: 1.456751\n",
            "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.443265\n",
            "Train Epoch: 2 [8320/50000 (17%)]\tLoss: 1.521233\n",
            "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.463179\n",
            "Train Epoch: 2 [9600/50000 (19%)]\tLoss: 1.287892\n",
            "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.367702\n",
            "Train Epoch: 2 [10880/50000 (22%)]\tLoss: 1.556870\n",
            "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.555519\n",
            "Train Epoch: 2 [12160/50000 (24%)]\tLoss: 1.116106\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.462569\n",
            "Train Epoch: 2 [13440/50000 (27%)]\tLoss: 1.433308\n",
            "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.346938\n",
            "Train Epoch: 2 [14720/50000 (29%)]\tLoss: 1.340392\n",
            "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.601002\n",
            "Train Epoch: 2 [16000/50000 (32%)]\tLoss: 1.358393\n",
            "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.636683\n",
            "Train Epoch: 2 [17280/50000 (35%)]\tLoss: 1.284813\n",
            "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.485621\n",
            "Train Epoch: 2 [18560/50000 (37%)]\tLoss: 1.597566\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.202954\n",
            "Train Epoch: 2 [19840/50000 (40%)]\tLoss: 1.392516\n",
            "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.667248\n",
            "Train Epoch: 2 [21120/50000 (42%)]\tLoss: 1.268927\n",
            "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 1.498563\n",
            "Train Epoch: 2 [22400/50000 (45%)]\tLoss: 1.426410\n",
            "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.430073\n",
            "Train Epoch: 2 [23680/50000 (47%)]\tLoss: 1.469121\n",
            "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.489567\n",
            "Train Epoch: 2 [24960/50000 (50%)]\tLoss: 1.482001\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.352811\n",
            "Train Epoch: 2 [26240/50000 (52%)]\tLoss: 1.532136\n",
            "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.421611\n",
            "Train Epoch: 2 [27520/50000 (55%)]\tLoss: 1.517575\n",
            "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.513230\n",
            "Train Epoch: 2 [28800/50000 (58%)]\tLoss: 1.312471\n",
            "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.408528\n",
            "Train Epoch: 2 [30080/50000 (60%)]\tLoss: 1.299995\n",
            "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.402754\n",
            "Train Epoch: 2 [31360/50000 (63%)]\tLoss: 1.379798\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.630858\n",
            "Train Epoch: 2 [32640/50000 (65%)]\tLoss: 1.432980\n",
            "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.695533\n",
            "Train Epoch: 2 [33920/50000 (68%)]\tLoss: 1.533964\n",
            "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.571724\n",
            "Train Epoch: 2 [35200/50000 (70%)]\tLoss: 1.490289\n",
            "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 1.182315\n",
            "Train Epoch: 2 [36480/50000 (73%)]\tLoss: 1.457879\n",
            "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.355869\n",
            "Train Epoch: 2 [37760/50000 (75%)]\tLoss: 1.301498\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.367727\n",
            "Train Epoch: 2 [39040/50000 (78%)]\tLoss: 1.400523\n",
            "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.439897\n",
            "Train Epoch: 2 [40320/50000 (81%)]\tLoss: 1.211211\n",
            "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.668530\n",
            "Train Epoch: 2 [41600/50000 (83%)]\tLoss: 1.556350\n",
            "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.228559\n",
            "Train Epoch: 2 [42880/50000 (86%)]\tLoss: 1.581538\n",
            "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.313356\n",
            "Train Epoch: 2 [44160/50000 (88%)]\tLoss: 1.555842\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.447757\n",
            "Train Epoch: 2 [45440/50000 (91%)]\tLoss: 1.329115\n",
            "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.512314\n",
            "Train Epoch: 2 [46720/50000 (93%)]\tLoss: 1.381582\n",
            "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.383133\n",
            "Train Epoch: 2 [48000/50000 (96%)]\tLoss: 1.519023\n",
            "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.432325\n",
            "Train Epoch: 2 [49280/50000 (98%)]\tLoss: 1.511799\n",
            "Train Epoch: 2 [49920/50000 (100%)]\tLoss: 1.292132\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.384945\n",
            "Train Epoch: 3 [640/50000 (1%)]\tLoss: 1.360810\n",
            "Train Epoch: 3 [1280/50000 (3%)]\tLoss: 1.417094\n",
            "Train Epoch: 3 [1920/50000 (4%)]\tLoss: 1.251364\n",
            "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 1.234432\n",
            "Train Epoch: 3 [3200/50000 (6%)]\tLoss: 1.343657\n",
            "Train Epoch: 3 [3840/50000 (8%)]\tLoss: 1.441974\n",
            "Train Epoch: 3 [4480/50000 (9%)]\tLoss: 1.171901\n",
            "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 1.262904\n",
            "Train Epoch: 3 [5760/50000 (12%)]\tLoss: 1.643705\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.335479\n",
            "Train Epoch: 3 [7040/50000 (14%)]\tLoss: 1.349391\n",
            "Train Epoch: 3 [7680/50000 (15%)]\tLoss: 1.498403\n",
            "Train Epoch: 3 [8320/50000 (17%)]\tLoss: 1.501292\n",
            "Train Epoch: 3 [8960/50000 (18%)]\tLoss: 1.372335\n",
            "Train Epoch: 3 [9600/50000 (19%)]\tLoss: 1.182101\n",
            "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 1.457623\n",
            "Train Epoch: 3 [10880/50000 (22%)]\tLoss: 1.231210\n",
            "Train Epoch: 3 [11520/50000 (23%)]\tLoss: 1.247492\n",
            "Train Epoch: 3 [12160/50000 (24%)]\tLoss: 1.408929\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.334991\n",
            "Train Epoch: 3 [13440/50000 (27%)]\tLoss: 1.310535\n",
            "Train Epoch: 3 [14080/50000 (28%)]\tLoss: 1.217648\n",
            "Train Epoch: 3 [14720/50000 (29%)]\tLoss: 1.458048\n",
            "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 1.322795\n",
            "Train Epoch: 3 [16000/50000 (32%)]\tLoss: 1.257250\n",
            "Train Epoch: 3 [16640/50000 (33%)]\tLoss: 1.377039\n",
            "Train Epoch: 3 [17280/50000 (35%)]\tLoss: 1.532952\n",
            "Train Epoch: 3 [17920/50000 (36%)]\tLoss: 1.194289\n",
            "Train Epoch: 3 [18560/50000 (37%)]\tLoss: 1.168759\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.300256\n",
            "Train Epoch: 3 [19840/50000 (40%)]\tLoss: 1.569297\n",
            "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 1.477742\n",
            "Train Epoch: 3 [21120/50000 (42%)]\tLoss: 1.208022\n",
            "Train Epoch: 3 [21760/50000 (43%)]\tLoss: 1.341708\n",
            "Train Epoch: 3 [22400/50000 (45%)]\tLoss: 1.292833\n",
            "Train Epoch: 3 [23040/50000 (46%)]\tLoss: 1.172891\n",
            "Train Epoch: 3 [23680/50000 (47%)]\tLoss: 1.369622\n",
            "Train Epoch: 3 [24320/50000 (49%)]\tLoss: 1.392646\n",
            "Train Epoch: 3 [24960/50000 (50%)]\tLoss: 1.354514\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.431704\n",
            "Train Epoch: 3 [26240/50000 (52%)]\tLoss: 1.221702\n",
            "Train Epoch: 3 [26880/50000 (54%)]\tLoss: 1.242776\n",
            "Train Epoch: 3 [27520/50000 (55%)]\tLoss: 1.330394\n",
            "Train Epoch: 3 [28160/50000 (56%)]\tLoss: 1.450190\n",
            "Train Epoch: 3 [28800/50000 (58%)]\tLoss: 1.398075\n",
            "Train Epoch: 3 [29440/50000 (59%)]\tLoss: 1.229728\n",
            "Train Epoch: 3 [30080/50000 (60%)]\tLoss: 1.452070\n",
            "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 1.452466\n",
            "Train Epoch: 3 [31360/50000 (63%)]\tLoss: 1.243468\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.340445\n",
            "Train Epoch: 3 [32640/50000 (65%)]\tLoss: 1.352730\n",
            "Train Epoch: 3 [33280/50000 (66%)]\tLoss: 1.203837\n",
            "Train Epoch: 3 [33920/50000 (68%)]\tLoss: 1.466549\n",
            "Train Epoch: 3 [34560/50000 (69%)]\tLoss: 1.356944\n",
            "Train Epoch: 3 [35200/50000 (70%)]\tLoss: 1.314930\n",
            "Train Epoch: 3 [35840/50000 (72%)]\tLoss: 1.436506\n",
            "Train Epoch: 3 [36480/50000 (73%)]\tLoss: 1.397658\n",
            "Train Epoch: 3 [37120/50000 (74%)]\tLoss: 1.363931\n",
            "Train Epoch: 3 [37760/50000 (75%)]\tLoss: 1.435687\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.436618\n",
            "Train Epoch: 3 [39040/50000 (78%)]\tLoss: 1.414066\n",
            "Train Epoch: 3 [39680/50000 (79%)]\tLoss: 1.491891\n",
            "Train Epoch: 3 [40320/50000 (81%)]\tLoss: 1.642274\n",
            "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 1.090529\n",
            "Train Epoch: 3 [41600/50000 (83%)]\tLoss: 1.631592\n",
            "Train Epoch: 3 [42240/50000 (84%)]\tLoss: 1.095932\n",
            "Train Epoch: 3 [42880/50000 (86%)]\tLoss: 1.298134\n",
            "Train Epoch: 3 [43520/50000 (87%)]\tLoss: 1.197421\n",
            "Train Epoch: 3 [44160/50000 (88%)]\tLoss: 1.219736\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.202537\n",
            "Train Epoch: 3 [45440/50000 (91%)]\tLoss: 1.154854\n",
            "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 1.323830\n",
            "Train Epoch: 3 [46720/50000 (93%)]\tLoss: 1.145138\n",
            "Train Epoch: 3 [47360/50000 (95%)]\tLoss: 1.373145\n",
            "Train Epoch: 3 [48000/50000 (96%)]\tLoss: 1.376369\n",
            "Train Epoch: 3 [48640/50000 (97%)]\tLoss: 1.353114\n",
            "Train Epoch: 3 [49280/50000 (98%)]\tLoss: 1.177377\n",
            "Train Epoch: 3 [49920/50000 (100%)]\tLoss: 1.282282\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 0.975233\n",
            "Train Epoch: 4 [640/50000 (1%)]\tLoss: 1.275937\n",
            "Train Epoch: 4 [1280/50000 (3%)]\tLoss: 1.493571\n",
            "Train Epoch: 4 [1920/50000 (4%)]\tLoss: 1.256508\n",
            "Train Epoch: 4 [2560/50000 (5%)]\tLoss: 1.267731\n",
            "Train Epoch: 4 [3200/50000 (6%)]\tLoss: 1.359849\n",
            "Train Epoch: 4 [3840/50000 (8%)]\tLoss: 1.401114\n",
            "Train Epoch: 4 [4480/50000 (9%)]\tLoss: 1.218058\n",
            "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 1.585964\n",
            "Train Epoch: 4 [5760/50000 (12%)]\tLoss: 1.118777\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.604015\n",
            "Train Epoch: 4 [7040/50000 (14%)]\tLoss: 1.379643\n",
            "Train Epoch: 4 [7680/50000 (15%)]\tLoss: 1.165693\n",
            "Train Epoch: 4 [8320/50000 (17%)]\tLoss: 1.331679\n",
            "Train Epoch: 4 [8960/50000 (18%)]\tLoss: 1.223200\n",
            "Train Epoch: 4 [9600/50000 (19%)]\tLoss: 1.341248\n",
            "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 1.361175\n",
            "Train Epoch: 4 [10880/50000 (22%)]\tLoss: 1.428425\n",
            "Train Epoch: 4 [11520/50000 (23%)]\tLoss: 1.405012\n",
            "Train Epoch: 4 [12160/50000 (24%)]\tLoss: 1.224887\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.194191\n",
            "Train Epoch: 4 [13440/50000 (27%)]\tLoss: 1.089339\n",
            "Train Epoch: 4 [14080/50000 (28%)]\tLoss: 1.409851\n",
            "Train Epoch: 4 [14720/50000 (29%)]\tLoss: 1.365703\n",
            "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 1.444391\n",
            "Train Epoch: 4 [16000/50000 (32%)]\tLoss: 1.176379\n",
            "Train Epoch: 4 [16640/50000 (33%)]\tLoss: 1.191084\n",
            "Train Epoch: 4 [17280/50000 (35%)]\tLoss: 1.435785\n",
            "Train Epoch: 4 [17920/50000 (36%)]\tLoss: 1.353510\n",
            "Train Epoch: 4 [18560/50000 (37%)]\tLoss: 1.450871\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.286801\n",
            "Train Epoch: 4 [19840/50000 (40%)]\tLoss: 1.362266\n",
            "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.348275\n",
            "Train Epoch: 4 [21120/50000 (42%)]\tLoss: 1.273656\n",
            "Train Epoch: 4 [21760/50000 (43%)]\tLoss: 1.228064\n",
            "Train Epoch: 4 [22400/50000 (45%)]\tLoss: 1.284031\n",
            "Train Epoch: 4 [23040/50000 (46%)]\tLoss: 1.282889\n",
            "Train Epoch: 4 [23680/50000 (47%)]\tLoss: 1.484811\n",
            "Train Epoch: 4 [24320/50000 (49%)]\tLoss: 1.474579\n",
            "Train Epoch: 4 [24960/50000 (50%)]\tLoss: 1.075700\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.434578\n",
            "Train Epoch: 4 [26240/50000 (52%)]\tLoss: 1.352023\n",
            "Train Epoch: 4 [26880/50000 (54%)]\tLoss: 1.361765\n",
            "Train Epoch: 4 [27520/50000 (55%)]\tLoss: 1.126577\n",
            "Train Epoch: 4 [28160/50000 (56%)]\tLoss: 1.035729\n",
            "Train Epoch: 4 [28800/50000 (58%)]\tLoss: 0.987819\n",
            "Train Epoch: 4 [29440/50000 (59%)]\tLoss: 1.311735\n",
            "Train Epoch: 4 [30080/50000 (60%)]\tLoss: 1.338863\n",
            "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.176930\n",
            "Train Epoch: 4 [31360/50000 (63%)]\tLoss: 1.154725\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.327197\n",
            "Train Epoch: 4 [32640/50000 (65%)]\tLoss: 1.348327\n",
            "Train Epoch: 4 [33280/50000 (66%)]\tLoss: 1.358181\n",
            "Train Epoch: 4 [33920/50000 (68%)]\tLoss: 1.370692\n",
            "Train Epoch: 4 [34560/50000 (69%)]\tLoss: 1.567757\n",
            "Train Epoch: 4 [35200/50000 (70%)]\tLoss: 1.328101\n",
            "Train Epoch: 4 [35840/50000 (72%)]\tLoss: 1.118650\n",
            "Train Epoch: 4 [36480/50000 (73%)]\tLoss: 1.323622\n",
            "Train Epoch: 4 [37120/50000 (74%)]\tLoss: 1.236609\n",
            "Train Epoch: 4 [37760/50000 (75%)]\tLoss: 1.469317\n",
            "Train Epoch: 4 [38400/50000 (77%)]\tLoss: 1.378279\n",
            "Train Epoch: 4 [39040/50000 (78%)]\tLoss: 1.179128\n",
            "Train Epoch: 4 [39680/50000 (79%)]\tLoss: 1.249413\n",
            "Train Epoch: 4 [40320/50000 (81%)]\tLoss: 1.408556\n",
            "Train Epoch: 4 [40960/50000 (82%)]\tLoss: 1.415303\n",
            "Train Epoch: 4 [41600/50000 (83%)]\tLoss: 1.335977\n",
            "Train Epoch: 4 [42240/50000 (84%)]\tLoss: 1.232674\n",
            "Train Epoch: 4 [42880/50000 (86%)]\tLoss: 1.067502\n",
            "Train Epoch: 4 [43520/50000 (87%)]\tLoss: 1.487782\n",
            "Train Epoch: 4 [44160/50000 (88%)]\tLoss: 1.397882\n",
            "Train Epoch: 4 [44800/50000 (90%)]\tLoss: 1.468890\n",
            "Train Epoch: 4 [45440/50000 (91%)]\tLoss: 1.122812\n",
            "Train Epoch: 4 [46080/50000 (92%)]\tLoss: 1.372395\n",
            "Train Epoch: 4 [46720/50000 (93%)]\tLoss: 1.192378\n",
            "Train Epoch: 4 [47360/50000 (95%)]\tLoss: 1.375124\n",
            "Train Epoch: 4 [48000/50000 (96%)]\tLoss: 1.479247\n",
            "Train Epoch: 4 [48640/50000 (97%)]\tLoss: 1.332142\n",
            "Train Epoch: 4 [49280/50000 (98%)]\tLoss: 1.528109\n",
            "Train Epoch: 4 [49920/50000 (100%)]\tLoss: 1.234256\n",
            "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1.248552\n",
            "Train Epoch: 5 [640/50000 (1%)]\tLoss: 1.169824\n",
            "Train Epoch: 5 [1280/50000 (3%)]\tLoss: 1.188165\n",
            "Train Epoch: 5 [1920/50000 (4%)]\tLoss: 1.316332\n",
            "Train Epoch: 5 [2560/50000 (5%)]\tLoss: 1.353862\n",
            "Train Epoch: 5 [3200/50000 (6%)]\tLoss: 1.229755\n",
            "Train Epoch: 5 [3840/50000 (8%)]\tLoss: 1.111338\n",
            "Train Epoch: 5 [4480/50000 (9%)]\tLoss: 1.184029\n",
            "Train Epoch: 5 [5120/50000 (10%)]\tLoss: 1.097673\n",
            "Train Epoch: 5 [5760/50000 (12%)]\tLoss: 1.265994\n",
            "Train Epoch: 5 [6400/50000 (13%)]\tLoss: 1.206549\n",
            "Train Epoch: 5 [7040/50000 (14%)]\tLoss: 1.230390\n",
            "Train Epoch: 5 [7680/50000 (15%)]\tLoss: 1.287362\n",
            "Train Epoch: 5 [8320/50000 (17%)]\tLoss: 1.328875\n",
            "Train Epoch: 5 [8960/50000 (18%)]\tLoss: 1.145323\n",
            "Train Epoch: 5 [9600/50000 (19%)]\tLoss: 1.014131\n",
            "Train Epoch: 5 [10240/50000 (20%)]\tLoss: 1.368008\n",
            "Train Epoch: 5 [10880/50000 (22%)]\tLoss: 1.432141\n",
            "Train Epoch: 5 [11520/50000 (23%)]\tLoss: 1.174241\n",
            "Train Epoch: 5 [12160/50000 (24%)]\tLoss: 1.178792\n",
            "Train Epoch: 5 [12800/50000 (26%)]\tLoss: 1.025932\n",
            "Train Epoch: 5 [13440/50000 (27%)]\tLoss: 0.972073\n",
            "Train Epoch: 5 [14080/50000 (28%)]\tLoss: 1.135049\n",
            "Train Epoch: 5 [14720/50000 (29%)]\tLoss: 1.084887\n",
            "Train Epoch: 5 [15360/50000 (31%)]\tLoss: 1.285201\n",
            "Train Epoch: 5 [16000/50000 (32%)]\tLoss: 1.291418\n",
            "Train Epoch: 5 [16640/50000 (33%)]\tLoss: 1.361042\n",
            "Train Epoch: 5 [17280/50000 (35%)]\tLoss: 1.179929\n",
            "Train Epoch: 5 [17920/50000 (36%)]\tLoss: 1.155825\n",
            "Train Epoch: 5 [18560/50000 (37%)]\tLoss: 1.004651\n",
            "Train Epoch: 5 [19200/50000 (38%)]\tLoss: 1.043156\n",
            "Train Epoch: 5 [19840/50000 (40%)]\tLoss: 1.199374\n",
            "Train Epoch: 5 [20480/50000 (41%)]\tLoss: 1.296377\n",
            "Train Epoch: 5 [21120/50000 (42%)]\tLoss: 1.067199\n",
            "Train Epoch: 5 [21760/50000 (43%)]\tLoss: 1.418340\n",
            "Train Epoch: 5 [22400/50000 (45%)]\tLoss: 1.328318\n",
            "Train Epoch: 5 [23040/50000 (46%)]\tLoss: 1.057790\n",
            "Train Epoch: 5 [23680/50000 (47%)]\tLoss: 0.913151\n",
            "Train Epoch: 5 [24320/50000 (49%)]\tLoss: 1.198340\n",
            "Train Epoch: 5 [24960/50000 (50%)]\tLoss: 1.177641\n",
            "Train Epoch: 5 [25600/50000 (51%)]\tLoss: 1.580797\n",
            "Train Epoch: 5 [26240/50000 (52%)]\tLoss: 1.365077\n",
            "Train Epoch: 5 [26880/50000 (54%)]\tLoss: 1.188745\n",
            "Train Epoch: 5 [27520/50000 (55%)]\tLoss: 0.996001\n",
            "Train Epoch: 5 [28160/50000 (56%)]\tLoss: 1.354059\n",
            "Train Epoch: 5 [28800/50000 (58%)]\tLoss: 1.199699\n",
            "Train Epoch: 5 [29440/50000 (59%)]\tLoss: 1.125767\n",
            "Train Epoch: 5 [30080/50000 (60%)]\tLoss: 1.065767\n",
            "Train Epoch: 5 [30720/50000 (61%)]\tLoss: 1.356508\n",
            "Train Epoch: 5 [31360/50000 (63%)]\tLoss: 1.166318\n",
            "Train Epoch: 5 [32000/50000 (64%)]\tLoss: 1.373978\n",
            "Train Epoch: 5 [32640/50000 (65%)]\tLoss: 1.050835\n",
            "Train Epoch: 5 [33280/50000 (66%)]\tLoss: 1.292637\n",
            "Train Epoch: 5 [33920/50000 (68%)]\tLoss: 1.206765\n",
            "Train Epoch: 5 [34560/50000 (69%)]\tLoss: 1.358742\n",
            "Train Epoch: 5 [35200/50000 (70%)]\tLoss: 0.990693\n",
            "Train Epoch: 5 [35840/50000 (72%)]\tLoss: 0.998391\n",
            "Train Epoch: 5 [36480/50000 (73%)]\tLoss: 1.354883\n",
            "Train Epoch: 5 [37120/50000 (74%)]\tLoss: 1.129098\n",
            "Train Epoch: 5 [37760/50000 (75%)]\tLoss: 1.102453\n",
            "Train Epoch: 5 [38400/50000 (77%)]\tLoss: 1.370007\n",
            "Train Epoch: 5 [39040/50000 (78%)]\tLoss: 1.106947\n",
            "Train Epoch: 5 [39680/50000 (79%)]\tLoss: 0.994140\n",
            "Train Epoch: 5 [40320/50000 (81%)]\tLoss: 0.947185\n",
            "Train Epoch: 5 [40960/50000 (82%)]\tLoss: 0.955870\n",
            "Train Epoch: 5 [41600/50000 (83%)]\tLoss: 1.192121\n",
            "Train Epoch: 5 [42240/50000 (84%)]\tLoss: 1.206711\n",
            "Train Epoch: 5 [42880/50000 (86%)]\tLoss: 1.279979\n",
            "Train Epoch: 5 [43520/50000 (87%)]\tLoss: 0.906890\n",
            "Train Epoch: 5 [44160/50000 (88%)]\tLoss: 1.218291\n",
            "Train Epoch: 5 [44800/50000 (90%)]\tLoss: 1.135840\n",
            "Train Epoch: 5 [45440/50000 (91%)]\tLoss: 1.035169\n",
            "Train Epoch: 5 [46080/50000 (92%)]\tLoss: 1.134881\n",
            "Train Epoch: 5 [46720/50000 (93%)]\tLoss: 1.192137\n",
            "Train Epoch: 5 [47360/50000 (95%)]\tLoss: 1.193899\n",
            "Train Epoch: 5 [48000/50000 (96%)]\tLoss: 1.281863\n",
            "Train Epoch: 5 [48640/50000 (97%)]\tLoss: 1.213736\n",
            "Train Epoch: 5 [49280/50000 (98%)]\tLoss: 1.299748\n",
            "Train Epoch: 5 [49920/50000 (100%)]\tLoss: 1.165587\n",
            "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1.327651\n",
            "Train Epoch: 6 [640/50000 (1%)]\tLoss: 1.330285\n",
            "Train Epoch: 6 [1280/50000 (3%)]\tLoss: 1.057517\n",
            "Train Epoch: 6 [1920/50000 (4%)]\tLoss: 1.350008\n",
            "Train Epoch: 6 [2560/50000 (5%)]\tLoss: 1.269223\n",
            "Train Epoch: 6 [3200/50000 (6%)]\tLoss: 1.103462\n",
            "Train Epoch: 6 [3840/50000 (8%)]\tLoss: 1.040710\n",
            "Train Epoch: 6 [4480/50000 (9%)]\tLoss: 0.934482\n",
            "Train Epoch: 6 [5120/50000 (10%)]\tLoss: 1.056842\n",
            "Train Epoch: 6 [5760/50000 (12%)]\tLoss: 1.198534\n",
            "Train Epoch: 6 [6400/50000 (13%)]\tLoss: 1.248119\n",
            "Train Epoch: 6 [7040/50000 (14%)]\tLoss: 1.402023\n",
            "Train Epoch: 6 [7680/50000 (15%)]\tLoss: 1.388007\n",
            "Train Epoch: 6 [8320/50000 (17%)]\tLoss: 0.948183\n",
            "Train Epoch: 6 [8960/50000 (18%)]\tLoss: 1.201061\n",
            "Train Epoch: 6 [9600/50000 (19%)]\tLoss: 1.153095\n",
            "Train Epoch: 6 [10240/50000 (20%)]\tLoss: 1.097138\n",
            "Train Epoch: 6 [10880/50000 (22%)]\tLoss: 1.136814\n",
            "Train Epoch: 6 [11520/50000 (23%)]\tLoss: 1.162121\n",
            "Train Epoch: 6 [12160/50000 (24%)]\tLoss: 1.263142\n",
            "Train Epoch: 6 [12800/50000 (26%)]\tLoss: 1.373051\n",
            "Train Epoch: 6 [13440/50000 (27%)]\tLoss: 1.150804\n",
            "Train Epoch: 6 [14080/50000 (28%)]\tLoss: 1.193618\n",
            "Train Epoch: 6 [14720/50000 (29%)]\tLoss: 0.810257\n",
            "Train Epoch: 6 [15360/50000 (31%)]\tLoss: 1.159660\n",
            "Train Epoch: 6 [16000/50000 (32%)]\tLoss: 1.358345\n",
            "Train Epoch: 6 [16640/50000 (33%)]\tLoss: 1.256785\n",
            "Train Epoch: 6 [17280/50000 (35%)]\tLoss: 1.193823\n",
            "Train Epoch: 6 [17920/50000 (36%)]\tLoss: 0.971029\n",
            "Train Epoch: 6 [18560/50000 (37%)]\tLoss: 1.227517\n",
            "Train Epoch: 6 [19200/50000 (38%)]\tLoss: 1.040581\n",
            "Train Epoch: 6 [19840/50000 (40%)]\tLoss: 1.204885\n",
            "Train Epoch: 6 [20480/50000 (41%)]\tLoss: 1.030405\n",
            "Train Epoch: 6 [21120/50000 (42%)]\tLoss: 1.523209\n",
            "Train Epoch: 6 [21760/50000 (43%)]\tLoss: 0.874430\n",
            "Train Epoch: 6 [22400/50000 (45%)]\tLoss: 1.033721\n",
            "Train Epoch: 6 [23040/50000 (46%)]\tLoss: 1.236778\n",
            "Train Epoch: 6 [23680/50000 (47%)]\tLoss: 1.100874\n",
            "Train Epoch: 6 [24320/50000 (49%)]\tLoss: 1.381946\n",
            "Train Epoch: 6 [24960/50000 (50%)]\tLoss: 1.190064\n",
            "Train Epoch: 6 [25600/50000 (51%)]\tLoss: 1.158800\n",
            "Train Epoch: 6 [26240/50000 (52%)]\tLoss: 1.153970\n",
            "Train Epoch: 6 [26880/50000 (54%)]\tLoss: 1.062623\n",
            "Train Epoch: 6 [27520/50000 (55%)]\tLoss: 1.212829\n",
            "Train Epoch: 6 [28160/50000 (56%)]\tLoss: 1.322768\n",
            "Train Epoch: 6 [28800/50000 (58%)]\tLoss: 1.205986\n",
            "Train Epoch: 6 [29440/50000 (59%)]\tLoss: 1.301443\n",
            "Train Epoch: 6 [30080/50000 (60%)]\tLoss: 0.969912\n",
            "Train Epoch: 6 [30720/50000 (61%)]\tLoss: 0.892122\n",
            "Train Epoch: 6 [31360/50000 (63%)]\tLoss: 0.948057\n",
            "Train Epoch: 6 [32000/50000 (64%)]\tLoss: 1.231305\n",
            "Train Epoch: 6 [32640/50000 (65%)]\tLoss: 1.112637\n",
            "Train Epoch: 6 [33280/50000 (66%)]\tLoss: 1.163057\n",
            "Train Epoch: 6 [33920/50000 (68%)]\tLoss: 1.044680\n",
            "Train Epoch: 6 [34560/50000 (69%)]\tLoss: 1.176978\n",
            "Train Epoch: 6 [35200/50000 (70%)]\tLoss: 1.002882\n",
            "Train Epoch: 6 [35840/50000 (72%)]\tLoss: 1.014045\n",
            "Train Epoch: 6 [36480/50000 (73%)]\tLoss: 1.089926\n",
            "Train Epoch: 6 [37120/50000 (74%)]\tLoss: 1.298235\n",
            "Train Epoch: 6 [37760/50000 (75%)]\tLoss: 0.912975\n",
            "Train Epoch: 6 [38400/50000 (77%)]\tLoss: 1.083475\n",
            "Train Epoch: 6 [39040/50000 (78%)]\tLoss: 1.344695\n",
            "Train Epoch: 6 [39680/50000 (79%)]\tLoss: 1.124683\n",
            "Train Epoch: 6 [40320/50000 (81%)]\tLoss: 1.324880\n",
            "Train Epoch: 6 [40960/50000 (82%)]\tLoss: 1.216006\n",
            "Train Epoch: 6 [41600/50000 (83%)]\tLoss: 1.018947\n",
            "Train Epoch: 6 [42240/50000 (84%)]\tLoss: 1.270400\n",
            "Train Epoch: 6 [42880/50000 (86%)]\tLoss: 1.412726\n",
            "Train Epoch: 6 [43520/50000 (87%)]\tLoss: 1.095995\n",
            "Train Epoch: 6 [44160/50000 (88%)]\tLoss: 1.051569\n",
            "Train Epoch: 6 [44800/50000 (90%)]\tLoss: 1.109813\n",
            "Train Epoch: 6 [45440/50000 (91%)]\tLoss: 1.151896\n",
            "Train Epoch: 6 [46080/50000 (92%)]\tLoss: 1.128115\n",
            "Train Epoch: 6 [46720/50000 (93%)]\tLoss: 1.131565\n",
            "Train Epoch: 6 [47360/50000 (95%)]\tLoss: 1.239266\n",
            "Train Epoch: 6 [48000/50000 (96%)]\tLoss: 1.212962\n",
            "Train Epoch: 6 [48640/50000 (97%)]\tLoss: 1.020590\n",
            "Train Epoch: 6 [49280/50000 (98%)]\tLoss: 1.124938\n",
            "Train Epoch: 6 [49920/50000 (100%)]\tLoss: 1.378036\n",
            "Train Epoch: 7 [0/50000 (0%)]\tLoss: 0.897388\n",
            "Train Epoch: 7 [640/50000 (1%)]\tLoss: 1.216371\n",
            "Train Epoch: 7 [1280/50000 (3%)]\tLoss: 1.440677\n",
            "Train Epoch: 7 [1920/50000 (4%)]\tLoss: 1.058713\n",
            "Train Epoch: 7 [2560/50000 (5%)]\tLoss: 1.202486\n",
            "Train Epoch: 7 [3200/50000 (6%)]\tLoss: 0.934932\n",
            "Train Epoch: 7 [3840/50000 (8%)]\tLoss: 1.152970\n",
            "Train Epoch: 7 [4480/50000 (9%)]\tLoss: 1.247809\n",
            "Train Epoch: 7 [5120/50000 (10%)]\tLoss: 1.191046\n",
            "Train Epoch: 7 [5760/50000 (12%)]\tLoss: 1.176339\n",
            "Train Epoch: 7 [6400/50000 (13%)]\tLoss: 0.760787\n",
            "Train Epoch: 7 [7040/50000 (14%)]\tLoss: 1.043579\n",
            "Train Epoch: 7 [7680/50000 (15%)]\tLoss: 1.067050\n",
            "Train Epoch: 7 [8320/50000 (17%)]\tLoss: 1.306404\n",
            "Train Epoch: 7 [8960/50000 (18%)]\tLoss: 1.022612\n",
            "Train Epoch: 7 [9600/50000 (19%)]\tLoss: 1.162972\n",
            "Train Epoch: 7 [10240/50000 (20%)]\tLoss: 1.004050\n",
            "Train Epoch: 7 [10880/50000 (22%)]\tLoss: 0.830513\n",
            "Train Epoch: 7 [11520/50000 (23%)]\tLoss: 0.772943\n",
            "Train Epoch: 7 [12160/50000 (24%)]\tLoss: 1.132848\n",
            "Train Epoch: 7 [12800/50000 (26%)]\tLoss: 0.806043\n",
            "Train Epoch: 7 [13440/50000 (27%)]\tLoss: 0.964378\n",
            "Train Epoch: 7 [14080/50000 (28%)]\tLoss: 1.268160\n",
            "Train Epoch: 7 [14720/50000 (29%)]\tLoss: 0.919813\n",
            "Train Epoch: 7 [15360/50000 (31%)]\tLoss: 1.042979\n",
            "Train Epoch: 7 [16000/50000 (32%)]\tLoss: 1.024336\n",
            "Train Epoch: 7 [16640/50000 (33%)]\tLoss: 1.074676\n",
            "Train Epoch: 7 [17280/50000 (35%)]\tLoss: 0.831348\n",
            "Train Epoch: 7 [17920/50000 (36%)]\tLoss: 0.877166\n",
            "Train Epoch: 7 [18560/50000 (37%)]\tLoss: 1.087270\n",
            "Train Epoch: 7 [19200/50000 (38%)]\tLoss: 1.127067\n",
            "Train Epoch: 7 [19840/50000 (40%)]\tLoss: 1.197314\n",
            "Train Epoch: 7 [20480/50000 (41%)]\tLoss: 1.006616\n",
            "Train Epoch: 7 [21120/50000 (42%)]\tLoss: 1.196598\n",
            "Train Epoch: 7 [21760/50000 (43%)]\tLoss: 0.977374\n",
            "Train Epoch: 7 [22400/50000 (45%)]\tLoss: 1.099877\n",
            "Train Epoch: 7 [23040/50000 (46%)]\tLoss: 1.080045\n",
            "Train Epoch: 7 [23680/50000 (47%)]\tLoss: 0.963876\n",
            "Train Epoch: 7 [24320/50000 (49%)]\tLoss: 1.083593\n",
            "Train Epoch: 7 [24960/50000 (50%)]\tLoss: 1.061911\n",
            "Train Epoch: 7 [25600/50000 (51%)]\tLoss: 1.296092\n",
            "Train Epoch: 7 [26240/50000 (52%)]\tLoss: 1.181564\n",
            "Train Epoch: 7 [26880/50000 (54%)]\tLoss: 1.081684\n",
            "Train Epoch: 7 [27520/50000 (55%)]\tLoss: 1.099942\n",
            "Train Epoch: 7 [28160/50000 (56%)]\tLoss: 1.228400\n",
            "Train Epoch: 7 [28800/50000 (58%)]\tLoss: 1.138497\n",
            "Train Epoch: 7 [29440/50000 (59%)]\tLoss: 1.161366\n",
            "Train Epoch: 7 [30080/50000 (60%)]\tLoss: 1.028795\n",
            "Train Epoch: 7 [30720/50000 (61%)]\tLoss: 0.966837\n",
            "Train Epoch: 7 [31360/50000 (63%)]\tLoss: 0.901263\n",
            "Train Epoch: 7 [32000/50000 (64%)]\tLoss: 1.017899\n",
            "Train Epoch: 7 [32640/50000 (65%)]\tLoss: 1.396147\n",
            "Train Epoch: 7 [33280/50000 (66%)]\tLoss: 0.842162\n",
            "Train Epoch: 7 [33920/50000 (68%)]\tLoss: 0.904623\n",
            "Train Epoch: 7 [34560/50000 (69%)]\tLoss: 1.225545\n",
            "Train Epoch: 7 [35200/50000 (70%)]\tLoss: 1.052055\n",
            "Train Epoch: 7 [35840/50000 (72%)]\tLoss: 0.994933\n",
            "Train Epoch: 7 [36480/50000 (73%)]\tLoss: 1.157603\n",
            "Train Epoch: 7 [37120/50000 (74%)]\tLoss: 1.352228\n",
            "Train Epoch: 7 [37760/50000 (75%)]\tLoss: 1.255591\n",
            "Train Epoch: 7 [38400/50000 (77%)]\tLoss: 0.971918\n",
            "Train Epoch: 7 [39040/50000 (78%)]\tLoss: 1.217724\n",
            "Train Epoch: 7 [39680/50000 (79%)]\tLoss: 1.087536\n",
            "Train Epoch: 7 [40320/50000 (81%)]\tLoss: 0.864522\n",
            "Train Epoch: 7 [40960/50000 (82%)]\tLoss: 1.017435\n",
            "Train Epoch: 7 [41600/50000 (83%)]\tLoss: 0.827263\n",
            "Train Epoch: 7 [42240/50000 (84%)]\tLoss: 0.883182\n",
            "Train Epoch: 7 [42880/50000 (86%)]\tLoss: 0.757345\n",
            "Train Epoch: 7 [43520/50000 (87%)]\tLoss: 1.053102\n",
            "Train Epoch: 7 [44160/50000 (88%)]\tLoss: 1.028079\n",
            "Train Epoch: 7 [44800/50000 (90%)]\tLoss: 1.440547\n",
            "Train Epoch: 7 [45440/50000 (91%)]\tLoss: 1.095601\n",
            "Train Epoch: 7 [46080/50000 (92%)]\tLoss: 1.078094\n",
            "Train Epoch: 7 [46720/50000 (93%)]\tLoss: 0.941348\n",
            "Train Epoch: 7 [47360/50000 (95%)]\tLoss: 1.255265\n",
            "Train Epoch: 7 [48000/50000 (96%)]\tLoss: 1.048330\n",
            "Train Epoch: 7 [48640/50000 (97%)]\tLoss: 1.440212\n",
            "Train Epoch: 7 [49280/50000 (98%)]\tLoss: 1.230736\n",
            "Train Epoch: 7 [49920/50000 (100%)]\tLoss: 0.990205\n",
            "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1.261853\n",
            "Train Epoch: 8 [640/50000 (1%)]\tLoss: 0.910282\n",
            "Train Epoch: 8 [1280/50000 (3%)]\tLoss: 0.755765\n",
            "Train Epoch: 8 [1920/50000 (4%)]\tLoss: 0.855523\n",
            "Train Epoch: 8 [2560/50000 (5%)]\tLoss: 1.191010\n",
            "Train Epoch: 8 [3200/50000 (6%)]\tLoss: 0.923356\n",
            "Train Epoch: 8 [3840/50000 (8%)]\tLoss: 0.964275\n",
            "Train Epoch: 8 [4480/50000 (9%)]\tLoss: 1.073873\n",
            "Train Epoch: 8 [5120/50000 (10%)]\tLoss: 1.057326\n",
            "Train Epoch: 8 [5760/50000 (12%)]\tLoss: 0.935275\n",
            "Train Epoch: 8 [6400/50000 (13%)]\tLoss: 0.959789\n",
            "Train Epoch: 8 [7040/50000 (14%)]\tLoss: 0.843205\n",
            "Train Epoch: 8 [7680/50000 (15%)]\tLoss: 0.925746\n",
            "Train Epoch: 8 [8320/50000 (17%)]\tLoss: 1.316838\n",
            "Train Epoch: 8 [8960/50000 (18%)]\tLoss: 0.776497\n",
            "Train Epoch: 8 [9600/50000 (19%)]\tLoss: 0.955863\n",
            "Train Epoch: 8 [10240/50000 (20%)]\tLoss: 1.043914\n",
            "Train Epoch: 8 [10880/50000 (22%)]\tLoss: 1.153527\n",
            "Train Epoch: 8 [11520/50000 (23%)]\tLoss: 0.998181\n",
            "Train Epoch: 8 [12160/50000 (24%)]\tLoss: 0.954075\n",
            "Train Epoch: 8 [12800/50000 (26%)]\tLoss: 1.138299\n",
            "Train Epoch: 8 [13440/50000 (27%)]\tLoss: 1.417588\n",
            "Train Epoch: 8 [14080/50000 (28%)]\tLoss: 1.026268\n",
            "Train Epoch: 8 [14720/50000 (29%)]\tLoss: 0.863865\n",
            "Train Epoch: 8 [15360/50000 (31%)]\tLoss: 1.013088\n",
            "Train Epoch: 8 [16000/50000 (32%)]\tLoss: 1.038519\n",
            "Train Epoch: 8 [16640/50000 (33%)]\tLoss: 1.171841\n",
            "Train Epoch: 8 [17280/50000 (35%)]\tLoss: 0.907643\n",
            "Train Epoch: 8 [17920/50000 (36%)]\tLoss: 0.984526\n",
            "Train Epoch: 8 [18560/50000 (37%)]\tLoss: 1.046894\n",
            "Train Epoch: 8 [19200/50000 (38%)]\tLoss: 1.190582\n",
            "Train Epoch: 8 [19840/50000 (40%)]\tLoss: 0.994777\n",
            "Train Epoch: 8 [20480/50000 (41%)]\tLoss: 1.014807\n",
            "Train Epoch: 8 [21120/50000 (42%)]\tLoss: 0.933389\n",
            "Train Epoch: 8 [21760/50000 (43%)]\tLoss: 1.011030\n",
            "Train Epoch: 8 [22400/50000 (45%)]\tLoss: 0.869318\n",
            "Train Epoch: 8 [23040/50000 (46%)]\tLoss: 1.183764\n",
            "Train Epoch: 8 [23680/50000 (47%)]\tLoss: 1.160721\n",
            "Train Epoch: 8 [24320/50000 (49%)]\tLoss: 1.319514\n",
            "Train Epoch: 8 [24960/50000 (50%)]\tLoss: 0.905169\n",
            "Train Epoch: 8 [25600/50000 (51%)]\tLoss: 0.811672\n",
            "Train Epoch: 8 [26240/50000 (52%)]\tLoss: 0.787133\n",
            "Train Epoch: 8 [26880/50000 (54%)]\tLoss: 0.928866\n",
            "Train Epoch: 8 [27520/50000 (55%)]\tLoss: 1.268597\n",
            "Train Epoch: 8 [28160/50000 (56%)]\tLoss: 0.986984\n",
            "Train Epoch: 8 [28800/50000 (58%)]\tLoss: 1.010419\n",
            "Train Epoch: 8 [29440/50000 (59%)]\tLoss: 1.093181\n",
            "Train Epoch: 8 [30080/50000 (60%)]\tLoss: 0.925171\n",
            "Train Epoch: 8 [30720/50000 (61%)]\tLoss: 0.874470\n",
            "Train Epoch: 8 [31360/50000 (63%)]\tLoss: 1.176825\n",
            "Train Epoch: 8 [32000/50000 (64%)]\tLoss: 0.999928\n",
            "Train Epoch: 8 [32640/50000 (65%)]\tLoss: 1.082583\n",
            "Train Epoch: 8 [33280/50000 (66%)]\tLoss: 1.046147\n",
            "Train Epoch: 8 [33920/50000 (68%)]\tLoss: 1.143109\n",
            "Train Epoch: 8 [34560/50000 (69%)]\tLoss: 0.813088\n",
            "Train Epoch: 8 [35200/50000 (70%)]\tLoss: 1.041373\n",
            "Train Epoch: 8 [35840/50000 (72%)]\tLoss: 0.898905\n",
            "Train Epoch: 8 [36480/50000 (73%)]\tLoss: 1.076604\n",
            "Train Epoch: 8 [37120/50000 (74%)]\tLoss: 0.822738\n",
            "Train Epoch: 8 [37760/50000 (75%)]\tLoss: 0.866564\n",
            "Train Epoch: 8 [38400/50000 (77%)]\tLoss: 0.937845\n",
            "Train Epoch: 8 [39040/50000 (78%)]\tLoss: 1.214830\n",
            "Train Epoch: 8 [39680/50000 (79%)]\tLoss: 1.262041\n",
            "Train Epoch: 8 [40320/50000 (81%)]\tLoss: 0.872190\n",
            "Train Epoch: 8 [40960/50000 (82%)]\tLoss: 0.983821\n",
            "Train Epoch: 8 [41600/50000 (83%)]\tLoss: 0.858261\n",
            "Train Epoch: 8 [42240/50000 (84%)]\tLoss: 1.052896\n",
            "Train Epoch: 8 [42880/50000 (86%)]\tLoss: 0.929760\n",
            "Train Epoch: 8 [43520/50000 (87%)]\tLoss: 0.747251\n",
            "Train Epoch: 8 [44160/50000 (88%)]\tLoss: 1.179165\n",
            "Train Epoch: 8 [44800/50000 (90%)]\tLoss: 0.918509\n",
            "Train Epoch: 8 [45440/50000 (91%)]\tLoss: 0.935065\n",
            "Train Epoch: 8 [46080/50000 (92%)]\tLoss: 0.904511\n",
            "Train Epoch: 8 [46720/50000 (93%)]\tLoss: 0.944425\n",
            "Train Epoch: 8 [47360/50000 (95%)]\tLoss: 1.084694\n",
            "Train Epoch: 8 [48000/50000 (96%)]\tLoss: 0.854043\n",
            "Train Epoch: 8 [48640/50000 (97%)]\tLoss: 0.732134\n",
            "Train Epoch: 8 [49280/50000 (98%)]\tLoss: 1.150218\n",
            "Train Epoch: 8 [49920/50000 (100%)]\tLoss: 1.053104\n",
            "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1.154188\n",
            "Train Epoch: 9 [640/50000 (1%)]\tLoss: 1.170700\n",
            "Train Epoch: 9 [1280/50000 (3%)]\tLoss: 0.798185\n",
            "Train Epoch: 9 [1920/50000 (4%)]\tLoss: 0.976120\n",
            "Train Epoch: 9 [2560/50000 (5%)]\tLoss: 1.246082\n",
            "Train Epoch: 9 [3200/50000 (6%)]\tLoss: 1.140565\n",
            "Train Epoch: 9 [3840/50000 (8%)]\tLoss: 0.955422\n",
            "Train Epoch: 9 [4480/50000 (9%)]\tLoss: 0.921871\n",
            "Train Epoch: 9 [5120/50000 (10%)]\tLoss: 0.895612\n",
            "Train Epoch: 9 [5760/50000 (12%)]\tLoss: 0.942861\n",
            "Train Epoch: 9 [6400/50000 (13%)]\tLoss: 1.051379\n",
            "Train Epoch: 9 [7040/50000 (14%)]\tLoss: 1.095063\n",
            "Train Epoch: 9 [7680/50000 (15%)]\tLoss: 0.942058\n",
            "Train Epoch: 9 [8320/50000 (17%)]\tLoss: 0.925631\n",
            "Train Epoch: 9 [8960/50000 (18%)]\tLoss: 0.921007\n",
            "Train Epoch: 9 [9600/50000 (19%)]\tLoss: 0.758821\n",
            "Train Epoch: 9 [10240/50000 (20%)]\tLoss: 0.995735\n",
            "Train Epoch: 9 [10880/50000 (22%)]\tLoss: 0.866099\n",
            "Train Epoch: 9 [11520/50000 (23%)]\tLoss: 0.951127\n",
            "Train Epoch: 9 [12160/50000 (24%)]\tLoss: 1.047183\n",
            "Train Epoch: 9 [12800/50000 (26%)]\tLoss: 0.986631\n",
            "Train Epoch: 9 [13440/50000 (27%)]\tLoss: 0.787278\n",
            "Train Epoch: 9 [14080/50000 (28%)]\tLoss: 0.779941\n",
            "Train Epoch: 9 [14720/50000 (29%)]\tLoss: 0.941234\n",
            "Train Epoch: 9 [15360/50000 (31%)]\tLoss: 1.042306\n",
            "Train Epoch: 9 [16000/50000 (32%)]\tLoss: 0.892808\n",
            "Train Epoch: 9 [16640/50000 (33%)]\tLoss: 0.964323\n",
            "Train Epoch: 9 [17280/50000 (35%)]\tLoss: 1.121162\n",
            "Train Epoch: 9 [17920/50000 (36%)]\tLoss: 1.222990\n",
            "Train Epoch: 9 [18560/50000 (37%)]\tLoss: 1.339598\n",
            "Train Epoch: 9 [19200/50000 (38%)]\tLoss: 0.736373\n",
            "Train Epoch: 9 [19840/50000 (40%)]\tLoss: 0.916724\n",
            "Train Epoch: 9 [20480/50000 (41%)]\tLoss: 0.924912\n",
            "Train Epoch: 9 [21120/50000 (42%)]\tLoss: 0.819100\n",
            "Train Epoch: 9 [21760/50000 (43%)]\tLoss: 0.865011\n",
            "Train Epoch: 9 [22400/50000 (45%)]\tLoss: 0.923259\n",
            "Train Epoch: 9 [23040/50000 (46%)]\tLoss: 0.888174\n",
            "Train Epoch: 9 [23680/50000 (47%)]\tLoss: 0.981485\n",
            "Train Epoch: 9 [24320/50000 (49%)]\tLoss: 0.918850\n",
            "Train Epoch: 9 [24960/50000 (50%)]\tLoss: 0.928013\n",
            "Train Epoch: 9 [25600/50000 (51%)]\tLoss: 1.041691\n",
            "Train Epoch: 9 [26240/50000 (52%)]\tLoss: 1.084167\n",
            "Train Epoch: 9 [26880/50000 (54%)]\tLoss: 1.013606\n",
            "Train Epoch: 9 [27520/50000 (55%)]\tLoss: 0.996958\n",
            "Train Epoch: 9 [28160/50000 (56%)]\tLoss: 1.212294\n",
            "Train Epoch: 9 [28800/50000 (58%)]\tLoss: 0.959765\n",
            "Train Epoch: 9 [29440/50000 (59%)]\tLoss: 0.830547\n",
            "Train Epoch: 9 [30080/50000 (60%)]\tLoss: 1.000822\n",
            "Train Epoch: 9 [30720/50000 (61%)]\tLoss: 1.044695\n",
            "Train Epoch: 9 [31360/50000 (63%)]\tLoss: 0.897761\n",
            "Train Epoch: 9 [32000/50000 (64%)]\tLoss: 1.098219\n",
            "Train Epoch: 9 [32640/50000 (65%)]\tLoss: 0.858607\n",
            "Train Epoch: 9 [33280/50000 (66%)]\tLoss: 0.829378\n",
            "Train Epoch: 9 [33920/50000 (68%)]\tLoss: 1.057724\n",
            "Train Epoch: 9 [34560/50000 (69%)]\tLoss: 0.967644\n",
            "Train Epoch: 9 [35200/50000 (70%)]\tLoss: 0.837015\n",
            "Train Epoch: 9 [35840/50000 (72%)]\tLoss: 0.834955\n",
            "Train Epoch: 9 [36480/50000 (73%)]\tLoss: 1.200546\n",
            "Train Epoch: 9 [37120/50000 (74%)]\tLoss: 0.871564\n",
            "Train Epoch: 9 [37760/50000 (75%)]\tLoss: 0.824052\n",
            "Train Epoch: 9 [38400/50000 (77%)]\tLoss: 1.071345\n",
            "Train Epoch: 9 [39040/50000 (78%)]\tLoss: 0.777535\n",
            "Train Epoch: 9 [39680/50000 (79%)]\tLoss: 0.835456\n",
            "Train Epoch: 9 [40320/50000 (81%)]\tLoss: 0.802992\n",
            "Train Epoch: 9 [40960/50000 (82%)]\tLoss: 0.789710\n",
            "Train Epoch: 9 [41600/50000 (83%)]\tLoss: 0.887795\n",
            "Train Epoch: 9 [42240/50000 (84%)]\tLoss: 1.123151\n",
            "Train Epoch: 9 [42880/50000 (86%)]\tLoss: 1.195382\n",
            "Train Epoch: 9 [43520/50000 (87%)]\tLoss: 0.980542\n",
            "Train Epoch: 9 [44160/50000 (88%)]\tLoss: 0.911622\n",
            "Train Epoch: 9 [44800/50000 (90%)]\tLoss: 0.808938\n",
            "Train Epoch: 9 [45440/50000 (91%)]\tLoss: 1.121208\n",
            "Train Epoch: 9 [46080/50000 (92%)]\tLoss: 0.971047\n",
            "Train Epoch: 9 [46720/50000 (93%)]\tLoss: 0.971970\n",
            "Train Epoch: 9 [47360/50000 (95%)]\tLoss: 1.114862\n",
            "Train Epoch: 9 [48000/50000 (96%)]\tLoss: 0.774306\n",
            "Train Epoch: 9 [48640/50000 (97%)]\tLoss: 1.168355\n",
            "Train Epoch: 9 [49280/50000 (98%)]\tLoss: 1.000167\n",
            "Train Epoch: 9 [49920/50000 (100%)]\tLoss: 1.087507\n",
            "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1.168678\n",
            "Train Epoch: 10 [640/50000 (1%)]\tLoss: 0.847442\n",
            "Train Epoch: 10 [1280/50000 (3%)]\tLoss: 1.118345\n",
            "Train Epoch: 10 [1920/50000 (4%)]\tLoss: 0.908776\n",
            "Train Epoch: 10 [2560/50000 (5%)]\tLoss: 0.803199\n",
            "Train Epoch: 10 [3200/50000 (6%)]\tLoss: 0.810847\n",
            "Train Epoch: 10 [3840/50000 (8%)]\tLoss: 1.059786\n",
            "Train Epoch: 10 [4480/50000 (9%)]\tLoss: 0.962223\n",
            "Train Epoch: 10 [5120/50000 (10%)]\tLoss: 0.805463\n",
            "Train Epoch: 10 [5760/50000 (12%)]\tLoss: 1.048858\n",
            "Train Epoch: 10 [6400/50000 (13%)]\tLoss: 0.890184\n",
            "Train Epoch: 10 [7040/50000 (14%)]\tLoss: 0.979570\n",
            "Train Epoch: 10 [7680/50000 (15%)]\tLoss: 0.985201\n",
            "Train Epoch: 10 [8320/50000 (17%)]\tLoss: 0.814421\n",
            "Train Epoch: 10 [8960/50000 (18%)]\tLoss: 0.914118\n",
            "Train Epoch: 10 [9600/50000 (19%)]\tLoss: 0.623442\n",
            "Train Epoch: 10 [10240/50000 (20%)]\tLoss: 1.077535\n",
            "Train Epoch: 10 [10880/50000 (22%)]\tLoss: 1.308757\n",
            "Train Epoch: 10 [11520/50000 (23%)]\tLoss: 0.913141\n",
            "Train Epoch: 10 [12160/50000 (24%)]\tLoss: 0.739887\n",
            "Train Epoch: 10 [12800/50000 (26%)]\tLoss: 0.739945\n",
            "Train Epoch: 10 [13440/50000 (27%)]\tLoss: 0.827095\n",
            "Train Epoch: 10 [14080/50000 (28%)]\tLoss: 0.661056\n",
            "Train Epoch: 10 [14720/50000 (29%)]\tLoss: 0.884202\n",
            "Train Epoch: 10 [15360/50000 (31%)]\tLoss: 1.075846\n",
            "Train Epoch: 10 [16000/50000 (32%)]\tLoss: 0.595007\n",
            "Train Epoch: 10 [16640/50000 (33%)]\tLoss: 0.859649\n",
            "Train Epoch: 10 [17280/50000 (35%)]\tLoss: 1.016238\n",
            "Train Epoch: 10 [17920/50000 (36%)]\tLoss: 0.961523\n",
            "Train Epoch: 10 [18560/50000 (37%)]\tLoss: 0.975719\n",
            "Train Epoch: 10 [19200/50000 (38%)]\tLoss: 1.065772\n",
            "Train Epoch: 10 [19840/50000 (40%)]\tLoss: 0.916018\n",
            "Train Epoch: 10 [20480/50000 (41%)]\tLoss: 0.836601\n",
            "Train Epoch: 10 [21120/50000 (42%)]\tLoss: 0.779427\n",
            "Train Epoch: 10 [21760/50000 (43%)]\tLoss: 0.957655\n",
            "Train Epoch: 10 [22400/50000 (45%)]\tLoss: 1.052466\n",
            "Train Epoch: 10 [23040/50000 (46%)]\tLoss: 0.934237\n",
            "Train Epoch: 10 [23680/50000 (47%)]\tLoss: 0.810291\n",
            "Train Epoch: 10 [24320/50000 (49%)]\tLoss: 0.749113\n",
            "Train Epoch: 10 [24960/50000 (50%)]\tLoss: 0.992491\n",
            "Train Epoch: 10 [25600/50000 (51%)]\tLoss: 0.826946\n",
            "Train Epoch: 10 [26240/50000 (52%)]\tLoss: 0.949386\n",
            "Train Epoch: 10 [26880/50000 (54%)]\tLoss: 1.030203\n",
            "Train Epoch: 10 [27520/50000 (55%)]\tLoss: 0.705505\n",
            "Train Epoch: 10 [28160/50000 (56%)]\tLoss: 0.901509\n",
            "Train Epoch: 10 [28800/50000 (58%)]\tLoss: 0.836746\n",
            "Train Epoch: 10 [29440/50000 (59%)]\tLoss: 0.804372\n",
            "Train Epoch: 10 [30080/50000 (60%)]\tLoss: 0.783290\n",
            "Train Epoch: 10 [30720/50000 (61%)]\tLoss: 0.871691\n",
            "Train Epoch: 10 [31360/50000 (63%)]\tLoss: 0.870730\n",
            "Train Epoch: 10 [32000/50000 (64%)]\tLoss: 1.112488\n",
            "Train Epoch: 10 [32640/50000 (65%)]\tLoss: 0.766180\n",
            "Train Epoch: 10 [33280/50000 (66%)]\tLoss: 0.992469\n",
            "Train Epoch: 10 [33920/50000 (68%)]\tLoss: 0.794342\n",
            "Train Epoch: 10 [34560/50000 (69%)]\tLoss: 1.033647\n",
            "Train Epoch: 10 [35200/50000 (70%)]\tLoss: 1.028224\n",
            "Train Epoch: 10 [35840/50000 (72%)]\tLoss: 0.844127\n",
            "Train Epoch: 10 [36480/50000 (73%)]\tLoss: 0.976151\n",
            "Train Epoch: 10 [37120/50000 (74%)]\tLoss: 1.085341\n",
            "Train Epoch: 10 [37760/50000 (75%)]\tLoss: 0.826340\n",
            "Train Epoch: 10 [38400/50000 (77%)]\tLoss: 0.867623\n",
            "Train Epoch: 10 [39040/50000 (78%)]\tLoss: 0.809669\n",
            "Train Epoch: 10 [39680/50000 (79%)]\tLoss: 0.879044\n",
            "Train Epoch: 10 [40320/50000 (81%)]\tLoss: 0.957006\n",
            "Train Epoch: 10 [40960/50000 (82%)]\tLoss: 1.062242\n",
            "Train Epoch: 10 [41600/50000 (83%)]\tLoss: 0.854724\n",
            "Train Epoch: 10 [42240/50000 (84%)]\tLoss: 0.934828\n",
            "Train Epoch: 10 [42880/50000 (86%)]\tLoss: 0.840467\n",
            "Train Epoch: 10 [43520/50000 (87%)]\tLoss: 1.039258\n",
            "Train Epoch: 10 [44160/50000 (88%)]\tLoss: 1.018979\n",
            "Train Epoch: 10 [44800/50000 (90%)]\tLoss: 0.910127\n",
            "Train Epoch: 10 [45440/50000 (91%)]\tLoss: 1.025439\n",
            "Train Epoch: 10 [46080/50000 (92%)]\tLoss: 0.808987\n",
            "Train Epoch: 10 [46720/50000 (93%)]\tLoss: 0.985170\n",
            "Train Epoch: 10 [47360/50000 (95%)]\tLoss: 0.695199\n",
            "Train Epoch: 10 [48000/50000 (96%)]\tLoss: 0.975081\n",
            "Train Epoch: 10 [48640/50000 (97%)]\tLoss: 0.935840\n",
            "Train Epoch: 10 [49280/50000 (98%)]\tLoss: 0.926338\n",
            "Train Epoch: 10 [49920/50000 (100%)]\tLoss: 0.768559\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model #= Net()\n",
        "#Model=model\n",
        "model.load_state_dict(torch.load('model.pth'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUjrJZpJ6jAV",
        "outputId": "43cb40a2-44ad-4e2f-9277-366241b9e751"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "test = x_test[70].reshape(1,3,32,32)\n",
        "print(y_test[70])\n",
        "test.shape\n",
        "test = test.type(torch.cuda.FloatTensor)\n",
        "print(torch.argmax(model(test),dim=1))#.dtype)\n",
        "plt.imshow(test.to(torch.uint8).cpu().numpy().reshape(32,32,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "YAHahUTtGGPP",
        "outputId": "74ba9f3e-adc5-4b79-ec7f-88175e2cb7ab"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2], dtype=torch.uint8)\n",
            "tensor([2], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6c381890d0>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAa6UlEQVR4nO2db4ycV3XGnzPvzOyu94/tjWNn66T516A2pSWgbUQFQhQEShFSQKoi+IDyIcKoImqR4EOUViWV+gFQAeVDRWWaiFBRQkqCiKqoJY2QIr4ENjQ4gbQlpKaJcewktvf/zs7Me/ph3tB1dJ+z69ndGcN9fpLl2Xvmvve+d94z78x95pxj7g4hxK8/tWFPQAgxGOTsQmSCnF2ITJCzC5EJcnYhMkHOLkQm1LfT2cxuAnA3gALAP7j7Z6Ln79s74ZcdnE7aumUwTi09zXqdT9/L4IAwaqk3GtRW1NM2L7u0T1lG0mZ/Ng+O6aSfBefcDeZfM34/KIK16rTb5Hh8HhFl2bngsQB+HVif84heseiI/Qjc/Rzv9MtnML+4lOzat7ObWQHg7wC8B8CLAH5gZg+7+09Yn8sOTuPo3Z9K2pZW+UXVHE2/QVxyyaW0T2t1ldqsVlDb9KUz1Lb/wCEy1jLts97i84DzN6Rul1/c7fV13o84Rc34OS8tLlDbyOgYtV1yML0eAPDKSyeT7c2REdrHwNdjZfEstZ09nR4LANZXV5Lt9UaT9onen6PfpVjwxtgN7mbsDTV6P2Jv6n/2l5/j4/DDbcqNAJ5z9+fdfR3A/QBu3sbxhBC7yHac/TCAFzb8/WLVJoS4CNn1DTozO2Jmc2Y2d25+abeHE0IQtuPsJwBcseHvy6u283D3o+4+6+6z+/ZObGM4IcR22I6z/wDAdWZ2tZk1AXwIwMM7My0hxE7T9268u3fM7HYA/4ae9Havu/847GRArZHeFW50+W5xQSS2TrBjvbh0jtquue53qe3g4av4PBrpneSR0VHaZ2VpntqWF/kco3NDjb9Hm6fXqigCtSOQ0EJ50/kxx8bH04ZACbFAAiwKPsdacMxGM/2a1YiMCiDcjo+k1EjxsFqkLae33SP52PqQWLels7v7IwAe2c4xhBCDQb+gEyIT5OxCZIKcXYhMkLMLkQlydiEyYVu78f1QI8pAoAyhXk8bG1GEWnDAMpB42q104AQAtNutZPvqEg8k6ZA+ALBGgjSAOEpqdHQPtTHpZXWFB+swaRPYLLKQryPtF0R3lODH8yAoxILXeqSeDuSJJLQgniWOToni1KLzJhJbGQTPMOktGkd3diEyQc4uRCbI2YXIBDm7EJkgZxciEwa7G+8lum2yK9whgRMA0E7v0hbBtulokwennHv1FLW1VhepjaUdWlrgu/HNJlcMLHivjQI/Wh2ec219fS3Z3gjSMCHYVV+a57v4Z4N1rDfSl9boGH+dy+i81rhyYUEASo0GDfFzjvbbu8EufjsIXopUDSNzbBRBH7Lrbkzugu7sQmSDnF2ITJCzC5EJcnYhMkHOLkQmyNmFyISBSm+lA621tOThQXWUznJa2mqtcZksehdbD6rPrMxz6YJaAjmmFVX16LcyVNgtvb79ll2KpKZukCON9Sui/HlREEcgs0b9Oqz8E+0BeGCNqr40SMDWZv36ealpZZrgYLqzC5EJcnYhMkHOLkQmyNmFyAQ5uxCZIGcXIhO2Jb2Z2XEAi+iFEHXcfTbs4AXKcn/SVARRWbSkTcmjjCxIWlaSKLpqMAqTjXhk1WYEOokFOdKCIzKJJxbeAskruELqzqPNmOQV5f/rdvk5d3Dh5ZMAoEaiwIqoDFVwvNAWvzKBJX3efaqvlJ3Q2f/I3V/ZgeMIIXYRfYwXIhO26+wO4Dtm9qSZHdmJCQkhdoftfox/u7ufMLODAB41s/9098c3PqF6EzgCAAcPTG9zOCFEv2zrzu7uJ6r/TwP4FoAbE8856u6z7j67d2piO8MJIbZB385uZuNmNvnaYwDvBfDMTk1MCLGzbOdj/CEA36qkiDqAf3L3fw17mKFWpCW2eqDxUDmJHAsAPCid0w3kHw8ED6bmhaWJoiikUMYJ3of7qTLUZ9miqFskDTWIMYqis1qQzLETlOwKEj2WpF/N+PVRL7gsF9lqgZwX5IHkySM3EUzJwaipb2d39+cBvKnf/kKIwSLpTYhMkLMLkQlydiEyQc4uRCbI2YXIhMHWegOoThXkLgRIxJMHMk6UzZHV1gJ4lBQAGNHRWJLHXqfA1GdYUyTJMEuURDEyRVOMXjLaL0i8GChXqEcJJ4PXbL29nmxvh/XyWtQWyWvN5gi1sdp3ANAgdeCKPqLvIrFOd3YhMkHOLkQmyNmFyAQ5uxCZIGcXIhMGuxvvQEl20GtBEAQ/HO8TBR4URbQjHJXwYYYgn1lU4ykI4EBQDivac3Xy/h2tVWQqgyifsHoVq04UrEeorgRr5cHOetFNKyXt1hrts7S0RG2tIH9hfWQPtU3t30dtI81Gsr3ZSLcDfAc/ek10ZxciE+TsQmSCnF2ITJCzC5EJcnYhMkHOLkQmDDwQxomkVJYXnm8rKsUTBX7E8k9UZojNIzjezoeSxJBzi1StiKhfX3FI0dpHueS6XF5rr61S2/+++FKyvahzWWtkhAe0jI6OUpvXeE7EdidarPR5t9t8PVjAVpRfUXd2ITJBzi5EJsjZhcgEObsQmSBnFyIT5OxCZMKm0puZ3Qvg/QBOu/sbq7ZpAN8AcBWA4wBucfezmx3LweWaKDqMSWWRhBbld3PnUWpFWN6HjBWohnEwX38lnqJyU06MoUwZHTAqhxXIlCWxhWJjh+d+66wsUNvK/Blqmz93LtkeBK9hfxChtm8ft3VKLg+ePH6K2qYvSVc3npoYp33q9fR1GgVZbuXO/hUAN72u7Q4Aj7n7dQAeq/4WQlzEbOrsVb3117913gzgvurxfQA+sMPzEkLsMP1+Zz/k7ierxy+hV9FVCHERs+0NOu99ceZpws2OmNmcmc0tLCxudzghRJ/06+ynzGwGAKr/T7MnuvtRd59199mpqck+hxNCbJd+nf1hALdWj28F8O2dmY4QYrfYivT2dQDvBHDAzF4E8GkAnwHwgJndBuDnAG7Z+pAXHn7FZKNIJoukvKhcUBSlxpJlRtJbpKHFclgUScdh8zeP3tejeQRjhRFxJPoumH30ekaRaM29e6ntDw5fm2xfCXJ9Li3yr5srKyvUthqUjVoN+p1qpUtU1S47SPscOngg3Se4pjZ1dnf/MDG9e7O+QoiLB/2CTohMkLMLkQlydiEyQc4uRCbI2YXIhIEnnGQ6VSQ1dTppnYS1B8NsSr3gS1In9bXisXicV6zYBbpWX+cW1LALbLH0FsibdBZ8rImptJwEAJOTPAKs0eCS3cJaOhJt/QyPoltb5AksO8YTVR68dD+1/cbMZdRmJOpwZISPNTqatkXSm+7sQmSCnF2ITJCzC5EJcnYhMkHOLkQmyNmFyISBSm8GLg1EEU+NBpEZarxPmLAxSJRoxt//6Hikfh0AtNs8Eqob1DaLZK3+IgcjmSxKAxke9cJ7sKydAIKlR3NkD7VNTvEkkI31dETZ0jJf+0ZjjNpqQXLOqfGgXxBpWRC5N5J0rY9rQHd2ITJBzi5EJsjZhcgEObsQmSBnFyITBh8IQ6gFu7Rsp54FpgDxrjrLJQcgjvxgXYJd6XrBgxnYLmxvGnyHvNvltYvKktj6zGkXbbj3s8O/eo6Xalo6+QK1vRJcHxOTPAddY086o3Hbm7TPKFF/ACBK5Rddw9H6l2VaGYhyFPKx+Di6swuRCXJ2ITJBzi5EJsjZhcgEObsQmSBnFyITtlL+6V4A7wdw2t3fWLXdBeCjAF6unnanuz+ylQGZAtEN8smtk2CGqE8URRAF3RSRfEIISzX1IeVtpx+TyiIZJ06hF+UGTL8uALA0fzbZ/vILXF5bePUValtbXaa26DpoddJS5FqNB9aMTE5T2+W/eQW1Tb3hamqLgrbYJRcGwkQ5Ctk4W3jOVwDclGj/orvfUP3bkqMLIYbHps7u7o8D4L+EEEL8SrCd7+y3m9kxM7vXzHgOXSHERUG/zv4lANcCuAHASQCfZ080syNmNmdmc/MLvBSuEGJ36cvZ3f2Uu3e99wPuLwO4MXjuUXefdffZvVPp3ykLIXafvpzdzGY2/PlBAM/szHSEELvFVqS3rwN4J4ADZvYigE8DeKeZ3YBeiM1xAB/bymAOp1FZUQkiptf1FxUUl3gqimAetHRVkMMtUEja7XRpIgAogzx5IWRJorx762s8T97qIv/qtXiW79uee/XlZHtrZYX3medjLa/ykkxlsP5GbGeX52kfb56jtqLJr503XHcltUVlmZjJgrx11BTIdZs6u7t/ONF8z2b9hBAXF/oFnRCZIGcXIhPk7EJkgpxdiEyQswuRCQNNOOkOdNrpCCWPJC8K1xnKLte82l0erdUO5A6WxDKSSKIyPWFkW2DrBFFe7VZaRlta4FLTysICP94ql8o8kA7rJFquEyQJXe1yCe3MCpcHizpPEHl4ZibZPnF4nPaZ2HcJtV1++WXUVm8Ekm5UjoxEsHlw7ZRRSBxBd3YhMkHOLkQmyNmFyAQ5uxCZIGcXIhPk7EJkwmBrvbmjQ6LeosglFgEWRYZF+fgsCg2KIo1IosoogWWEd7h01V7jktfqIpfKVojE1lnn0lU0DyfJPgGg1eK2VdJvbT2Q60h9OADYP8GlsrLGL+PjvziVbG9OcXntdw4e5vPYt4/aikhfC6455hOtQGLtEGmzGyQI1Z1diEyQswuRCXJ2ITJBzi5EJsjZhciEwe7Gg+9JRjnjGo10oANrBwD3sHYONwXzcLJb3Gmt0T6dYFd9bYHnOltb5IEra8tL1La+kp7LwhIvnxTl8kOgeLSCnfr1bnqHeXLvXtpncnKK2haWuZpw4izPXTffSs//mr18V33v1AS1NRuR8sLXMboeWf7FugXuWUufV6QH6M4uRCbI2YXIBDm7EJkgZxciE+TsQmSCnF2ITNhK+acrAHwVwCH0ihkddfe7zWwawDcAXIVeCahb3P1sfDAu80TyD/tpf5fIO70+F55LDkAoNZUkd916IJOtz79KbatBv3Nn+VKeORfkkyPSWyQpTk5wqWl0ZJTaiuYItY2T4JTaHj7WUosHfsw7f63X63uo7frfe0Oy/cor0rnpAODQwWlqGxnhLuNlJL3xABUWmNUMAnyaxF9qwbW9lTt7B8An3f16AG8F8HEzux7AHQAec/frADxW/S2EuEjZ1Nnd/aS7/7B6vAjgWQCHAdwM4L7qafcB+MBuTVIIsX0u6Du7mV0F4M0AngBwyN1PVqaX0PuYL4S4SNmys5vZBIAHAXzC3c/LnuC9LyTJLyVmdsTM5sxsbmGB/8xTCLG7bMnZzayBnqN/zd0fqppPmdlMZZ8BcDrV192Puvusu89OBb85FkLsLps6u/W2ye8B8Ky7f2GD6WEAt1aPbwXw7Z2fnhBip9hK1NvbAHwEwNNm9lTVdieAzwB4wMxuA/BzALdsfiijElBUQonlmlsnpaQAoBOUJupG/aKcayvpryHLZ8/QPrVAMmqv82i5FT59YGSSmqan0/nTxsa4hDY+yY/XaDaprQzuFUUj3S/KFze6xk96z6X8dbkyiCgbGUvLgwf280+Z43u4pFgEZcrCal5RKScm9wbH43PgnTZ1dnf/Hnjk3LsvfDpCiGGgX9AJkQlydiEyQc4uRCbI2YXIBDm7EJkw0ISTZVliZW01aVsLSgktLqaTJc7P80SDrTWeoHA0kJOadb4kxkpU1fjxxqd4EsWJMR6tNRPYRka5rTE6lmyPEnrWgySKUbRcu8NlxTUioy0t8V9RFl1uGw1kqKjs0h4io42P8WSlwSmjG5xzlOwxMjLpzYMITCaxRdKb7uxCZIKcXYhMkLMLkQlydiEyQc4uRCbI2YXIhIFLb0vL6dpnZ87yumdnXk0nWFxd5XJdFOU1NcWlq4mJcWprUFmOyx2NICljvcmXn4suwCKJvgOADkli2VrlUmSn5HJSUfA5MpkPAOr1tBzZWuU158ouj0YcafL70tgoX+OJ8fRrzZI8AkCnzdejDBKjFkHkZiOQN1ly1CjoLZLYGLqzC5EJcnYhMkHOLkQmyNmFyAQ5uxCZMNDdeDOjO9rT+/bTfnsn07ZGg+/C7hmP8ojxndEyKCnVJnnt2i2+091dT6sPANBa47vPXRZ0g7h8Vb2ePu/lFR40xM4LACYmea62fWM8yKdOXucCPADFgsuxHrxmzUagGBBbtHNe40MhmAZqFmgozq8RI9MPLkWuJgQBN7qzC5EJcnYhMkHOLkQmyNmFyAQ5uxCZIGcXIhM2ld7M7AoAX0WvJLMDOOrud5vZXQA+CuDl6ql3uvsj8bF4jrexIKjCLK13RLEAtRo3WhBmEsouRCMZrfP3zJoF8wjknygvXFEP8qeRtWoGZYsQBHeM7eGvS3OEy5udTlpW3DPG8/UhkBujdaw3uITZaKSP2WzysYI0hOF1VXa5hLk4zwO92PoXNV6Wq1ZPy55RHryt6OwdAJ909x+a2SSAJ83s0cr2RXf/2y0cQwgxZLZS6+0kgJPV40UzexZAunqgEOKi5YK+s5vZVQDeDOCJqul2MztmZveaGf8JnBBi6GzZ2c1sAsCDAD7h7gsAvgTgWgA3oHfn/zzpd8TM5sxsjuV/F0LsPltydjNroOfoX3P3hwDA3U+5e9fdSwBfBnBjqq+7H3X3WXefnZzkWWCEELvLps5uZgbgHgDPuvsXNrTPbHjaBwE8s/PTE0LsFFvZjX8bgI8AeNrMnqra7gTwYTO7AT057jiAj212ILMaRhpp6aUI9A4n0kQ3CgsKdDkLbFHJHRoRFxyvCCSvRlBnqBHIckUg/zjSc9w/yfPu1QIpL5Ll2oHU1KinJcBajctkRWQzbqvVgpJMRLIz53MvO9E9MMgl1+WvSzeIjCw9PV7Z4OdVq6dtHmSu28pu/PeQlu9CTV0IcXGhX9AJkQlydiEyQc4uRCbI2YXIBDm7EJkw2ISTAGpEiqoTqQYAaiTLn1kUQRXIU4GtDKS3LonkYu0AYEEkV7vN+3UCWTGS84zIeYGCBngkYfKxggBB1Mh5NwsueTXrXJ6KZLlISmWJO6MINSb1AgBqUVJMfj3um5ymtlYrvcbrgQS4tpwuAVZ2g8hBahFC/FohZxciE+TsQmSCnF2ITJCzC5EJcnYhMmGg0pu700SEUaa8gkhDUc22IsgcWQtqpdUC2YVaIuknULW6gcwXRlBFEX0kIi5S3iyYB4sa69n4UZniWHb52ndZ0TMglACDAEHUammJrbAgUi6qAxdcV5F8XIzxyMLGSnq85eWo7mDaZkHUm+7sQmSCnF2ITJCzC5EJcnYhMkHOLkQmyNmFyITBSm9wdDok2iiIDnMisXkR9QmSF0Z1zwJYMr8oyV+keVkg44RSWRSV5SQ5Z7C+cL5WEZFEVZDoOw/OzJ1HjQXl9GLprRgl7XuD4wWvi3NbWUaFArmtU09fP+16EBVZX0+2ezB33dmFyAQ5uxCZIGcXIhPk7EJkgpxdiEzYdDfezEYBPA5gpHr+N93902Z2NYD7AVwC4EkAH3H39Bbha8eC0QCVWlAKiW0+Rz/6D+I3wrxwUeCHE1utFux0B7vP8a56XyZaooqWrkJc8ipayFoUMMJ2n6PgmShIJgiEiRPssZx8PDDFCm4rAluYRzHIXfeLpbVk+/+c5q/L2np6Hqud4DWhlv+nBeBd7v4m9Moz32RmbwXwWQBfdPffAnAWwG1bOJYQYkhs6uze47VUlo3qnwN4F4BvVu33AfjArsxQCLEjbLU+e1FVcD0N4FEAPwNwzv2Xv8Z4EcDh3ZmiEGIn2JKzu3vX3W8AcDmAGwH89lYHMLMjZjZnZnMLi8t9TlMIsV0uaDfe3c8B+C6APwSwz+yXqUUuB3CC9Dnq7rPuPjs1Ob6tyQoh+mdTZzezS81sX/V4DMB7ADyLntP/SfW0WwF8e7cmKYTYPlsJhJkBcJ+ZFei9OTzg7v9iZj8BcL+Z/Q2A/wBwz+aHcjgrx1NG5ZpIe6QYBVIe+lPKUJKJRCV3ykgy6pOofJWXaYnNWQASgG6UKC8aqwhkRSJDRfJaoBqFMmVkYwE5RSCTNZpcXus2ebBOszFCbfU6X2MWeNOc5ME6VqbPq1YPylNRS4W7HwPw5kT78+h9fxdC/AqgX9AJkQlydiEyQc4uRCbI2YXIBDm7EJlgkYyz44OZvQzg59WfBwC8MrDBOZrH+Wge5/OrNo8r3f3SlGGgzn7ewGZz7j47lME1D80jw3noY7wQmSBnFyIThunsR4c49kY0j/PRPM7n12YeQ/vOLoQYLPoYL0QmDMXZzewmM/svM3vOzO4YxhyqeRw3s6fN7CkzmxvguPea2Wkze2ZD27SZPWpmP63+3z+kedxlZieqNXnKzN43gHlcYWbfNbOfmNmPzezPq/aBrkkwj4GuiZmNmtn3zexH1Tz+umq/2syeqPzmG2bGQ/BSuPtA/wEo0EtrdQ2AJoAfAbh+0POo5nIcwIEhjPsOAG8B8MyGts8BuKN6fAeAzw5pHncB+NSA12MGwFuqx5MA/hvA9YNek2AeA10T9AKtJ6rHDQBPAHgrgAcAfKhq/3sAf3ohxx3Gnf1GAM+5+/PeSz19P4CbhzCPoeHujwM487rmm9FL3AkMKIEnmcfAcfeT7v7D6vEieslRDmPAaxLMY6B4jx1P8joMZz8M4IUNfw8zWaUD+I6ZPWlmR4Y0h9c45O4nq8cvATg0xLncbmbHqo/5u/51YiNmdhV6+ROewBDX5HXzAAa8JruR5DX3Dbq3u/tbAPwxgI+b2TuGPSGg986OuBbEbvIlANeiVyPgJIDPD2pgM5sA8CCAT7j7wkbbINckMY+Br4lvI8krYxjOfgLAFRv+pskqdxt3P1H9fxrAtzDczDunzGwGAKr/Tw9jEu5+qrrQSgBfxoDWxHolWh4E8DV3f6hqHviapOYxrDWpxr7gJK+MYTj7DwBcV+0sNgF8CMDDg56EmY2b2eRrjwG8F8Azca9d5WH0EncCQ0zg+ZpzVXwQA1gT6yWRuwfAs+7+hQ2mga4Jm8eg12TXkrwOaofxdbuN70Nvp/NnAP5iSHO4Bj0l4EcAfjzIeQD4OnofB9voffe6Db2aeY8B+CmAfwcwPaR5/COApwEcQ8/ZZgYwj7ej9xH9GICnqn/vG/SaBPMY6JoA+H30krgeQ++N5a82XLPfB/AcgH8GMHIhx9Uv6ITIhNw36ITIBjm7EJkgZxciE+TsQmSCnF2ITJCzC5EJcnYhMkHOLkQm/B/PHT4LiXWuVwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "test = x_test[50].reshape(1,3,32,32)\n",
        "print(y_test[50])\n",
        "test.shape\n",
        "test = test.type(torch.cuda.FloatTensor)\n",
        "print(torch.argmax(model(test),dim=1))#.dtype)\n",
        "plt.imshow(test.to(torch.uint8).cpu().numpy().reshape(32,32,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "D8uXuaB6J3SU",
        "outputId": "7d45d28b-94bb-4d2f-c1e1-73cf1088297f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([9], dtype=torch.uint8)\n",
            "tensor([9], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6c38061dd0>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Fb6Ho9oL-0j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}